{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-setup.py_0-25", "title": "amazon-science_patchcore-inspection-setup.py", "text": "from pathlib import Path\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\npackage_path = Path(__file__).parent\n\nversion_path = Path(__file__).parent.joinpath(f\"src/patchcore/VERSION\")\nversion = version_path.read_text().strip()\ninstall_requires = (\n    Path(__file__).parent.joinpath(\"requirements.txt\").read_text().splitlines()\n)\ndata_files = [\n    str(version_path.relative_to(package_path)),\n]\n\nsetup(\n    name=\"patchcore\",\n    version=version,\n    install_requires=install_requires,\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    data_files=data_files,\n)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "setup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "setup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "setup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "\"\"\"Anomaly metrics.\"\"\"\nimport numpy as np\nfrom sklearn import metrics\n\n\ndef compute_imagewise_retrieval_metrics(\n    anomaly_prediction_weights, anomaly_ground_truth_labels\n):\n    \"\"\"\n    Computes retrieval statistics (AUROC, FPR, TPR).\n\n    Args:\n        anomaly_prediction_weights: [np.array or list] [N] Assignment weights\n                                    per image. Higher indicates higher\n                                    probability of being an anomaly.\n        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n                                    if image is an anomaly, 0 if not.\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    auroc = metrics.roc_auc_score(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "\"\"\"Anomaly metrics.\"\"\"\nimport numpy as np\nfrom sklearn import metrics\n\n\ndef compute_imagewise_retrieval_metrics(\n    anomaly_prediction_weights, anomaly_ground_truth_labels\n):\n    \"\"\"\n    Computes retrieval statistics (AUROC, FPR, TPR).\n\n    Args:\n        anomaly_prediction_weights: [np.array or list] [N] Assignment weights\n                                    per image. Higher indicates higher\n                                    probability of being an anomaly.\n        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n                                    if image is an anomaly, 0 if not.\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    auroc = metrics.roc_auc_score(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}\n\n\ndef compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n    \"\"\"\n    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n    and ground truth segmentation masks.\n\n    Args:\n        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n                                generated segmentation masks.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "\"\"\"Anomaly metrics.\"\"\"\nimport numpy as np\nfrom sklearn import metrics\n\n\ndef compute_imagewise_retrieval_metrics(\n    anomaly_prediction_weights, anomaly_ground_truth_labels\n):\n    \"\"\"\n    Computes retrieval statistics (AUROC, FPR, TPR).\n\n    Args:\n        anomaly_prediction_weights: [np.array or list] [N] Assignment weights\n                                    per image. Higher indicates higher\n                                    probability of being an anomaly.\n        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n                                    if image is an anomaly, 0 if not.\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    auroc = metrics.roc_auc_score(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}\n\n\ndef compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n    \"\"\"\n    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n    and ground truth segmentation masks.\n\n    Args:\n        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n                                generated segmentation masks.\n        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n                            predefined ground truth segmentation masks\n    \"\"\"\n    if isinstance(anomaly_segmentations, list):\n        anomaly_segmentations = np.stack(anomaly_segmentations)\n    if isinstance(ground_truth_masks, list):\n        ground_truth_masks = np.stack(ground_truth_masks)\n\n    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n    flat_ground_truth_masks = ground_truth_masks.ravel()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "def compute_imagewise_retrieval_metrics(\n    anomaly_prediction_weights, anomaly_ground_truth_labels\n):\n    \"\"\"\n    Computes retrieval statistics (AUROC, FPR, TPR).\n\n    Args:\n        anomaly_prediction_weights: [np.array or list] [N] Assignment weights\n                                    per image. Higher indicates higher\n                                    probability of being an anomaly.\n        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n                                    if image is an anomaly, 0 if not.\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    auroc = metrics.roc_auc_score(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}\n\n\ndef compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n    \"\"\"\n    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n    and ground truth segmentation masks.\n\n    Args:\n        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n                                generated segmentation masks.\n        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n                            predefined ground truth segmentation masks\n    \"\"\"\n    if isinstance(anomaly_segmentations, list):\n        anomaly_segmentations = np.stack(anomaly_segmentations)\n    if isinstance(ground_truth_masks, list):\n        ground_truth_masks = np.stack(ground_truth_masks)\n\n    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n    flat_ground_truth_masks = ground_truth_masks.ravel()\n\n    fpr, tpr, thresholds = metrics.roc_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    auroc = metrics.roc_auc_score(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n\n    precision, recall, thresholds = metrics.precision_recall_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n                                    if image is an anomaly, 0 if not.\n    \"\"\"\n    fpr, tpr, thresholds = metrics.roc_curve(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    auroc = metrics.roc_auc_score(\n        anomaly_ground_truth_labels, anomaly_prediction_weights\n    )\n    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}\n\n\ndef compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n    \"\"\"\n    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n    and ground truth segmentation masks.\n\n    Args:\n        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n                                generated segmentation masks.\n        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n                            predefined ground truth segmentation masks\n    \"\"\"\n    if isinstance(anomaly_segmentations, list):\n        anomaly_segmentations = np.stack(anomaly_segmentations)\n    if isinstance(ground_truth_masks, list):\n        ground_truth_masks = np.stack(ground_truth_masks)\n\n    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n    flat_ground_truth_masks = ground_truth_masks.ravel()\n\n    fpr, tpr, thresholds = metrics.roc_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    auroc = metrics.roc_auc_score(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n\n    precision, recall, thresholds = metrics.precision_recall_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    F1_scores = np.divide(\n        2 * precision * recall,\n        precision + recall,\n        out=np.zeros_like(precision),\n        where=(precision + recall) != 0,\n    )\n\n    optimal_threshold = thresholds[np.argmax(F1_scores)]\n    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "\n\ndef compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n    \"\"\"\n    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n    and ground truth segmentation masks.\n\n    Args:\n        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n                                generated segmentation masks.\n        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n                            predefined ground truth segmentation masks\n    \"\"\"\n    if isinstance(anomaly_segmentations, list):\n        anomaly_segmentations = np.stack(anomaly_segmentations)\n    if isinstance(ground_truth_masks, list):\n        ground_truth_masks = np.stack(ground_truth_masks)\n\n    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n    flat_ground_truth_masks = ground_truth_masks.ravel()\n\n    fpr, tpr, thresholds = metrics.roc_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    auroc = metrics.roc_auc_score(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n\n    precision, recall, thresholds = metrics.precision_recall_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    F1_scores = np.divide(\n        2 * precision * recall,\n        precision + recall,\n        out=np.zeros_like(precision),\n        where=(precision + recall) != 0,\n    )\n\n    optimal_threshold = thresholds[np.argmax(F1_scores)]\n    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)\n    fpr_optim = np.mean(predictions > flat_ground_truth_masks)\n    fnr_optim = np.mean(predictions < flat_ground_truth_masks)\n\n    return {\n        \"auroc\": auroc,\n        \"fpr\": fpr,\n        \"tpr\": tpr,\n        \"optimal_threshold\": optimal_threshold,\n        \"optimal_fpr\": fpr_optim,\n        \"optimal_fnr\": fnr_optim,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_35-76", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n                            predefined ground truth segmentation masks\n    \"\"\"\n    if isinstance(anomaly_segmentations, list):\n        anomaly_segmentations = np.stack(anomaly_segmentations)\n    if isinstance(ground_truth_masks, list):\n        ground_truth_masks = np.stack(ground_truth_masks)\n\n    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n    flat_ground_truth_masks = ground_truth_masks.ravel()\n\n    fpr, tpr, thresholds = metrics.roc_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    auroc = metrics.roc_auc_score(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n\n    precision, recall, thresholds = metrics.precision_recall_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    F1_scores = np.divide(\n        2 * precision * recall,\n        precision + recall,\n        out=np.zeros_like(precision),\n        where=(precision + recall) != 0,\n    )\n\n    optimal_threshold = thresholds[np.argmax(F1_scores)]\n    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)\n    fpr_optim = np.mean(predictions > flat_ground_truth_masks)\n    fnr_optim = np.mean(predictions < flat_ground_truth_masks)\n\n    return {\n        \"auroc\": auroc,\n        \"fpr\": fpr,\n        \"tpr\": tpr,\n        \"optimal_threshold\": optimal_threshold,\n        \"optimal_fpr\": fpr_optim,\n        \"optimal_fnr\": fnr_optim,\n    }", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 76, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 76, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-metrics.py_45-76", "title": "amazon-science_patchcore-inspection-src-patchcore-metrics.py", "text": "\n    fpr, tpr, thresholds = metrics.roc_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    auroc = metrics.roc_auc_score(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n\n    precision, recall, thresholds = metrics.precision_recall_curve(\n        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n    )\n    F1_scores = np.divide(\n        2 * precision * recall,\n        precision + recall,\n        out=np.zeros_like(precision),\n        where=(precision + recall) != 0,\n    )\n\n    optimal_threshold = thresholds[np.argmax(F1_scores)]\n    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)\n    fpr_optim = np.mean(predictions > flat_ground_truth_masks)\n    fnr_optim = np.mean(predictions < flat_ground_truth_masks)\n\n    return {\n        \"auroc\": auroc,\n        \"fpr\": fpr,\n        \"tpr\": tpr,\n        \"optimal_threshold\": optimal_threshold,\n        \"optimal_fpr\": fpr_optim,\n        \"optimal_fnr\": fnr_optim,\n    }", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "metrics.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 76, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "metrics.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 76, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "import abc\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "import abc\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "import abc\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()\n        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "import tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()\n        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n\n    def _reduce_features(self, features):\n        if features.shape[1] == self.dimension_to_project_features_to:\n            return features\n        mapper = torch.nn.Linear(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "class BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()\n        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n\n    def _reduce_features(self, features):\n        if features.shape[1] == self.dimension_to_project_features_to:\n            return features\n        mapper = torch.nn.Linear(\n            features.shape[1], self.dimension_to_project_features_to, bias=False\n        )\n        _ = mapper.to(self.device)\n        features = features.to(self.device)\n        return mapper(features)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Subsamples features using Greedy Coreset.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()\n        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n\n    def _reduce_features(self, features):\n        if features.shape[1] == self.dimension_to_project_features_to:\n            return features\n        mapper = torch.nn.Linear(\n            features.shape[1], self.dimension_to_project_features_to, bias=False\n        )\n        _ = mapper.to(self.device)\n        features = features.to(self.device)\n        return mapper(features)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Subsamples features using Greedy Coreset.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        if self.percentage == 1:\n            return features\n        self._store_type(features)\n        if isinstance(features, np.ndarray):\n            features = torch.from_numpy(features)\n        reduced_features = self._reduce_features(features)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_35-85", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n\n    def _reduce_features(self, features):\n        if features.shape[1] == self.dimension_to_project_features_to:\n            return features\n        mapper = torch.nn.Linear(\n            features.shape[1], self.dimension_to_project_features_to, bias=False\n        )\n        _ = mapper.to(self.device)\n        features = features.to(self.device)\n        return mapper(features)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Subsamples features using Greedy Coreset.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        if self.percentage == 1:\n            return features\n        self._store_type(features)\n        if isinstance(features, np.ndarray):\n            features = torch.from_numpy(features)\n        reduced_features = self._reduce_features(features)\n        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n        features = features[sample_indices]\n        return self._restore_type(features)\n\n    @staticmethod\n    def _compute_batchwise_differences(\n        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_45-95", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n\n    def _reduce_features(self, features):\n        if features.shape[1] == self.dimension_to_project_features_to:\n            return features\n        mapper = torch.nn.Linear(\n            features.shape[1], self.dimension_to_project_features_to, bias=False\n        )\n        _ = mapper.to(self.device)\n        features = features.to(self.device)\n        return mapper(features)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Subsamples features using Greedy Coreset.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        if self.percentage == 1:\n            return features\n        self._store_type(features)\n        if isinstance(features, np.ndarray):\n            features = torch.from_numpy(features)\n        reduced_features = self._reduce_features(features)\n        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n        features = features[sample_indices]\n        return self._restore_type(features)\n\n    @staticmethod\n    def _compute_batchwise_differences(\n        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)\n        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n        a_times_b = matrix_a.mm(matrix_b.T)\n\n        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs iterative greedy coreset selection.\n\n        Args:\n            features: [NxD] input feature bank to sample.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_55-105", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "            features.shape[1], self.dimension_to_project_features_to, bias=False\n        )\n        _ = mapper.to(self.device)\n        features = features.to(self.device)\n        return mapper(features)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Subsamples features using Greedy Coreset.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        if self.percentage == 1:\n            return features\n        self._store_type(features)\n        if isinstance(features, np.ndarray):\n            features = torch.from_numpy(features)\n        reduced_features = self._reduce_features(features)\n        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n        features = features[sample_indices]\n        return self._restore_type(features)\n\n    @staticmethod\n    def _compute_batchwise_differences(\n        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)\n        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n        a_times_b = matrix_a.mm(matrix_b.T)\n\n        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs iterative greedy coreset selection.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        distance_matrix = self._compute_batchwise_differences(features, features)\n        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        for _ in range(num_coreset_samples):\n            select_idx = torch.argmax(coreset_anchor_distances).item()\n            coreset_indices.append(select_idx)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_65-115", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "\n        Args:\n            features: [N x D]\n        \"\"\"\n        if self.percentage == 1:\n            return features\n        self._store_type(features)\n        if isinstance(features, np.ndarray):\n            features = torch.from_numpy(features)\n        reduced_features = self._reduce_features(features)\n        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n        features = features[sample_indices]\n        return self._restore_type(features)\n\n    @staticmethod\n    def _compute_batchwise_differences(\n        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)\n        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n        a_times_b = matrix_a.mm(matrix_b.T)\n\n        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs iterative greedy coreset selection.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        distance_matrix = self._compute_batchwise_differences(features, features)\n        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        for _ in range(num_coreset_samples):\n            select_idx = torch.argmax(coreset_anchor_distances).item()\n            coreset_indices.append(select_idx)\n\n            coreset_select_distance = distance_matrix[\n                :, select_idx : select_idx + 1  # noqa E203\n            ]\n            coreset_anchor_distances = torch.cat(\n                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n            )\n            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n\n        return np.array(coreset_indices)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_75-125", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n        features = features[sample_indices]\n        return self._restore_type(features)\n\n    @staticmethod\n    def _compute_batchwise_differences(\n        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)\n        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n        a_times_b = matrix_a.mm(matrix_b.T)\n\n        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs iterative greedy coreset selection.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        distance_matrix = self._compute_batchwise_differences(features, features)\n        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        for _ in range(num_coreset_samples):\n            select_idx = torch.argmax(coreset_anchor_distances).item()\n            coreset_indices.append(select_idx)\n\n            coreset_select_distance = distance_matrix[\n                :, select_idx : select_idx + 1  # noqa E203\n            ]\n            coreset_anchor_distances = torch.cat(\n                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n            )\n            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n\n        return np.array(coreset_indices)\n\n\nclass ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        number_of_starting_points: int = 10,\n        dimension_to_project_features_to: int = 128,\n    ):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_85-135", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n        a_times_b = matrix_a.mm(matrix_b.T)\n\n        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs iterative greedy coreset selection.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        distance_matrix = self._compute_batchwise_differences(features, features)\n        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        for _ in range(num_coreset_samples):\n            select_idx = torch.argmax(coreset_anchor_distances).item()\n            coreset_indices.append(select_idx)\n\n            coreset_select_distance = distance_matrix[\n                :, select_idx : select_idx + 1  # noqa E203\n            ]\n            coreset_anchor_distances = torch.cat(\n                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n            )\n            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n\n        return np.array(coreset_indices)\n\n\nclass ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        number_of_starting_points: int = 10,\n        dimension_to_project_features_to: int = 128,\n    ):\n        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n        self.number_of_starting_points = number_of_starting_points\n        super().__init__(percentage, device, dimension_to_project_features_to)\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs approximate iterative greedy coreset selection.\n\n        This greedy coreset implementation does not require computation of the\n        full N x N distance matrix and thus requires a lot less memory, however\n        at the cost of increased sampling times.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_95-145", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        \"\"\"\n        distance_matrix = self._compute_batchwise_differences(features, features)\n        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        for _ in range(num_coreset_samples):\n            select_idx = torch.argmax(coreset_anchor_distances).item()\n            coreset_indices.append(select_idx)\n\n            coreset_select_distance = distance_matrix[\n                :, select_idx : select_idx + 1  # noqa E203\n            ]\n            coreset_anchor_distances = torch.cat(\n                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n            )\n            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n\n        return np.array(coreset_indices)\n\n\nclass ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        number_of_starting_points: int = 10,\n        dimension_to_project_features_to: int = 128,\n    ):\n        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n        self.number_of_starting_points = number_of_starting_points\n        super().__init__(percentage, device, dimension_to_project_features_to)\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs approximate iterative greedy coreset selection.\n\n        This greedy coreset implementation does not require computation of the\n        full N x N distance matrix and thus requires a lot less memory, however\n        at the cost of increased sampling times.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        number_of_starting_points = np.clip(\n            self.number_of_starting_points, None, len(features)\n        )\n        start_points = np.random.choice(\n            len(features), number_of_starting_points, replace=False\n        ).tolist()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_105-155", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "\n            coreset_select_distance = distance_matrix[\n                :, select_idx : select_idx + 1  # noqa E203\n            ]\n            coreset_anchor_distances = torch.cat(\n                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n            )\n            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n\n        return np.array(coreset_indices)\n\n\nclass ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        number_of_starting_points: int = 10,\n        dimension_to_project_features_to: int = 128,\n    ):\n        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n        self.number_of_starting_points = number_of_starting_points\n        super().__init__(percentage, device, dimension_to_project_features_to)\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs approximate iterative greedy coreset selection.\n\n        This greedy coreset implementation does not require computation of the\n        full N x N distance matrix and thus requires a lot less memory, however\n        at the cost of increased sampling times.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        number_of_starting_points = np.clip(\n            self.number_of_starting_points, None, len(features)\n        )\n        start_points = np.random.choice(\n            len(features), number_of_starting_points, replace=False\n        ).tolist()\n\n        approximate_distance_matrix = self._compute_batchwise_differences(\n            features, features[start_points]\n        )\n        approximate_coreset_anchor_distances = torch.mean(\n            approximate_distance_matrix, axis=-1\n        ).reshape(-1, 1)\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_115-165", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "\n\nclass ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        number_of_starting_points: int = 10,\n        dimension_to_project_features_to: int = 128,\n    ):\n        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n        self.number_of_starting_points = number_of_starting_points\n        super().__init__(percentage, device, dimension_to_project_features_to)\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs approximate iterative greedy coreset selection.\n\n        This greedy coreset implementation does not require computation of the\n        full N x N distance matrix and thus requires a lot less memory, however\n        at the cost of increased sampling times.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        number_of_starting_points = np.clip(\n            self.number_of_starting_points, None, len(features)\n        )\n        start_points = np.random.choice(\n            len(features), number_of_starting_points, replace=False\n        ).tolist()\n\n        approximate_distance_matrix = self._compute_batchwise_differences(\n            features, features[start_points]\n        )\n        approximate_coreset_anchor_distances = torch.mean(\n            approximate_distance_matrix, axis=-1\n        ).reshape(-1, 1)\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        with torch.no_grad():\n            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n                coreset_indices.append(select_idx)\n                coreset_select_distance = self._compute_batchwise_differences(\n                    features, features[select_idx : select_idx + 1]  # noqa: E203\n                )\n                approximate_coreset_anchor_distances = torch.cat(\n                    [approximate_coreset_anchor_distances, coreset_select_distance],\n                    dim=-1,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_125-175", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n        self.number_of_starting_points = number_of_starting_points\n        super().__init__(percentage, device, dimension_to_project_features_to)\n\n    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n        \"\"\"Runs approximate iterative greedy coreset selection.\n\n        This greedy coreset implementation does not require computation of the\n        full N x N distance matrix and thus requires a lot less memory, however\n        at the cost of increased sampling times.\n\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        number_of_starting_points = np.clip(\n            self.number_of_starting_points, None, len(features)\n        )\n        start_points = np.random.choice(\n            len(features), number_of_starting_points, replace=False\n        ).tolist()\n\n        approximate_distance_matrix = self._compute_batchwise_differences(\n            features, features[start_points]\n        )\n        approximate_coreset_anchor_distances = torch.mean(\n            approximate_distance_matrix, axis=-1\n        ).reshape(-1, 1)\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        with torch.no_grad():\n            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n                coreset_indices.append(select_idx)\n                coreset_select_distance = self._compute_batchwise_differences(\n                    features, features[select_idx : select_idx + 1]  # noqa: E203\n                )\n                approximate_coreset_anchor_distances = torch.cat(\n                    [approximate_coreset_anchor_distances, coreset_select_distance],\n                    dim=-1,\n                )\n                approximate_coreset_anchor_distances = torch.min(\n                    approximate_coreset_anchor_distances, dim=1\n                ).values.reshape(-1, 1)\n\n        return np.array(coreset_indices)\n\n\nclass RandomSampler(BaseSampler):\n    def __init__(self, percentage: float):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_135-185", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "\n        Args:\n            features: [NxD] input feature bank to sample.\n        \"\"\"\n        number_of_starting_points = np.clip(\n            self.number_of_starting_points, None, len(features)\n        )\n        start_points = np.random.choice(\n            len(features), number_of_starting_points, replace=False\n        ).tolist()\n\n        approximate_distance_matrix = self._compute_batchwise_differences(\n            features, features[start_points]\n        )\n        approximate_coreset_anchor_distances = torch.mean(\n            approximate_distance_matrix, axis=-1\n        ).reshape(-1, 1)\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        with torch.no_grad():\n            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n                coreset_indices.append(select_idx)\n                coreset_select_distance = self._compute_batchwise_differences(\n                    features, features[select_idx : select_idx + 1]  # noqa: E203\n                )\n                approximate_coreset_anchor_distances = torch.cat(\n                    [approximate_coreset_anchor_distances, coreset_select_distance],\n                    dim=-1,\n                )\n                approximate_coreset_anchor_distances = torch.min(\n                    approximate_coreset_anchor_distances, dim=1\n                ).values.reshape(-1, 1)\n\n        return np.array(coreset_indices)\n\n\nclass RandomSampler(BaseSampler):\n    def __init__(self, percentage: float):\n        super().__init__(percentage)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Randomly samples input feature collection.\n\n        Args:\n            features: [N x D]\n        \"\"\"", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_145-191", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "\n        approximate_distance_matrix = self._compute_batchwise_differences(\n            features, features[start_points]\n        )\n        approximate_coreset_anchor_distances = torch.mean(\n            approximate_distance_matrix, axis=-1\n        ).reshape(-1, 1)\n        coreset_indices = []\n        num_coreset_samples = int(len(features) * self.percentage)\n\n        with torch.no_grad():\n            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n                coreset_indices.append(select_idx)\n                coreset_select_distance = self._compute_batchwise_differences(\n                    features, features[select_idx : select_idx + 1]  # noqa: E203\n                )\n                approximate_coreset_anchor_distances = torch.cat(\n                    [approximate_coreset_anchor_distances, coreset_select_distance],\n                    dim=-1,\n                )\n                approximate_coreset_anchor_distances = torch.min(\n                    approximate_coreset_anchor_distances, dim=1\n                ).values.reshape(-1, 1)\n\n        return np.array(coreset_indices)\n\n\nclass RandomSampler(BaseSampler):\n    def __init__(self, percentage: float):\n        super().__init__(percentage)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Randomly samples input feature collection.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        num_random_samples = int(len(features) * self.percentage)\n        subset_indices = np.random.choice(\n            len(features), num_random_samples, replace=False\n        )\n        subset_indices = np.array(subset_indices)\n        return features[subset_indices]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_155-191", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "        with torch.no_grad():\n            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n                coreset_indices.append(select_idx)\n                coreset_select_distance = self._compute_batchwise_differences(\n                    features, features[select_idx : select_idx + 1]  # noqa: E203\n                )\n                approximate_coreset_anchor_distances = torch.cat(\n                    [approximate_coreset_anchor_distances, coreset_select_distance],\n                    dim=-1,\n                )\n                approximate_coreset_anchor_distances = torch.min(\n                    approximate_coreset_anchor_distances, dim=1\n                ).values.reshape(-1, 1)\n\n        return np.array(coreset_indices)\n\n\nclass RandomSampler(BaseSampler):\n    def __init__(self, percentage: float):\n        super().__init__(percentage)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Randomly samples input feature collection.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        num_random_samples = int(len(features) * self.percentage)\n        subset_indices = np.random.choice(\n            len(features), num_random_samples, replace=False\n        )\n        subset_indices = np.array(subset_indices)\n        return features[subset_indices]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-sampler.py_165-191", "title": "amazon-science_patchcore-inspection-src-patchcore-sampler.py", "text": "                )\n                approximate_coreset_anchor_distances = torch.min(\n                    approximate_coreset_anchor_distances, dim=1\n                ).values.reshape(-1, 1)\n\n        return np.array(coreset_indices)\n\n\nclass RandomSampler(BaseSampler):\n    def __init__(self, percentage: float):\n        super().__init__(percentage)\n\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        \"\"\"Randomly samples input feature collection.\n\n        Args:\n            features: [N x D]\n        \"\"\"\n        num_random_samples = int(len(features) * self.percentage)\n        subset_indices = np.random.choice(\n            len(features), num_random_samples, replace=False\n        )\n        subset_indices = np.array(subset_indices)\n        return features[subset_indices]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "sampler.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 191, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "import csv\nimport logging\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport torch\nimport tqdm\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef plot_segmentation_images(\n    savefolder,\n    image_paths,\n    segmentations,\n    anomaly_scores=None,\n    mask_paths=None,\n    image_transform=lambda x: x,\n    mask_transform=lambda x: x,\n    save_depth=4,\n):\n    \"\"\"Generate anomaly segmentation images.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "import csv\nimport logging\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport torch\nimport tqdm\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef plot_segmentation_images(\n    savefolder,\n    image_paths,\n    segmentations,\n    anomaly_scores=None,\n    mask_paths=None,\n    image_transform=lambda x: x,\n    mask_transform=lambda x: x,\n    save_depth=4,\n):\n    \"\"\"Generate anomaly segmentation images.\n\n    Args:\n        image_paths: List[str] List of paths to images.\n        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n        anomaly_scores: [List[float]] Anomaly scores for each image.\n        mask_paths: [List[str]] List of paths to ground truth masks.\n        image_transform: [function or lambda] Optional transformation of images.\n        mask_transform: [function or lambda] Optional transformation of masks.\n        save_depth: [int] Number of path-strings to use for image savenames.\n    \"\"\"", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "import csv\nimport logging\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport torch\nimport tqdm\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef plot_segmentation_images(\n    savefolder,\n    image_paths,\n    segmentations,\n    anomaly_scores=None,\n    mask_paths=None,\n    image_transform=lambda x: x,\n    mask_transform=lambda x: x,\n    save_depth=4,\n):\n    \"\"\"Generate anomaly segmentation images.\n\n    Args:\n        image_paths: List[str] List of paths to images.\n        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n        anomaly_scores: [List[float]] Anomaly scores for each image.\n        mask_paths: [List[str]] List of paths to ground truth masks.\n        image_transform: [function or lambda] Optional transformation of images.\n        mask_transform: [function or lambda] Optional transformation of masks.\n        save_depth: [int] Number of path-strings to use for image savenames.\n    \"\"\"\n    if mask_paths is None:\n        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n    masks_provided = mask_paths[0] != \"-1\"\n    if anomaly_scores is None:\n        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n\n    os.makedirs(savefolder, exist_ok=True)\n\n    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n        zip(image_paths, mask_paths, anomaly_scores, segmentations),", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport torch\nimport tqdm\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef plot_segmentation_images(\n    savefolder,\n    image_paths,\n    segmentations,\n    anomaly_scores=None,\n    mask_paths=None,\n    image_transform=lambda x: x,\n    mask_transform=lambda x: x,\n    save_depth=4,\n):\n    \"\"\"Generate anomaly segmentation images.\n\n    Args:\n        image_paths: List[str] List of paths to images.\n        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n        anomaly_scores: [List[float]] Anomaly scores for each image.\n        mask_paths: [List[str]] List of paths to ground truth masks.\n        image_transform: [function or lambda] Optional transformation of images.\n        mask_transform: [function or lambda] Optional transformation of masks.\n        save_depth: [int] Number of path-strings to use for image savenames.\n    \"\"\"\n    if mask_paths is None:\n        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n    masks_provided = mask_paths[0] != \"-1\"\n    if anomaly_scores is None:\n        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n\n    os.makedirs(savefolder, exist_ok=True)\n\n    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n        zip(image_paths, mask_paths, anomaly_scores, segmentations),\n        total=len(image_paths),\n        desc=\"Generating Segmentation Images...\",\n        leave=False,\n    ):\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = image_transform(image)\n        if not isinstance(image, np.ndarray):\n            image = image.numpy()\n\n        if masks_provided:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "    savefolder,\n    image_paths,\n    segmentations,\n    anomaly_scores=None,\n    mask_paths=None,\n    image_transform=lambda x: x,\n    mask_transform=lambda x: x,\n    save_depth=4,\n):\n    \"\"\"Generate anomaly segmentation images.\n\n    Args:\n        image_paths: List[str] List of paths to images.\n        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n        anomaly_scores: [List[float]] Anomaly scores for each image.\n        mask_paths: [List[str]] List of paths to ground truth masks.\n        image_transform: [function or lambda] Optional transformation of images.\n        mask_transform: [function or lambda] Optional transformation of masks.\n        save_depth: [int] Number of path-strings to use for image savenames.\n    \"\"\"\n    if mask_paths is None:\n        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n    masks_provided = mask_paths[0] != \"-1\"\n    if anomaly_scores is None:\n        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n\n    os.makedirs(savefolder, exist_ok=True)\n\n    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n        zip(image_paths, mask_paths, anomaly_scores, segmentations),\n        total=len(image_paths),\n        desc=\"Generating Segmentation Images...\",\n        leave=False,\n    ):\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = image_transform(image)\n        if not isinstance(image, np.ndarray):\n            image = image.numpy()\n\n        if masks_provided:\n            if mask_path is not None:\n                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n                mask = mask_transform(mask)\n                if not isinstance(mask, np.ndarray):\n                    mask = mask.numpy()\n            else:\n                mask = np.zeros_like(image)\n\n        savename = image_path.split(\"/\")\n        savename = \"_\".join(savename[-save_depth:])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "\n    Args:\n        image_paths: List[str] List of paths to images.\n        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n        anomaly_scores: [List[float]] Anomaly scores for each image.\n        mask_paths: [List[str]] List of paths to ground truth masks.\n        image_transform: [function or lambda] Optional transformation of images.\n        mask_transform: [function or lambda] Optional transformation of masks.\n        save_depth: [int] Number of path-strings to use for image savenames.\n    \"\"\"\n    if mask_paths is None:\n        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n    masks_provided = mask_paths[0] != \"-1\"\n    if anomaly_scores is None:\n        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n\n    os.makedirs(savefolder, exist_ok=True)\n\n    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n        zip(image_paths, mask_paths, anomaly_scores, segmentations),\n        total=len(image_paths),\n        desc=\"Generating Segmentation Images...\",\n        leave=False,\n    ):\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = image_transform(image)\n        if not isinstance(image, np.ndarray):\n            image = image.numpy()\n\n        if masks_provided:\n            if mask_path is not None:\n                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n                mask = mask_transform(mask)\n                if not isinstance(mask, np.ndarray):\n                    mask = mask.numpy()\n            else:\n                mask = np.zeros_like(image)\n\n        savename = image_path.split(\"/\")\n        savename = \"_\".join(savename[-save_depth:])\n        savename = os.path.join(savefolder, savename)\n        f, axes = plt.subplots(1, 2 + int(masks_provided))\n        axes[0].imshow(image.transpose(1, 2, 0))\n        axes[1].imshow(mask.transpose(1, 2, 0))\n        axes[2].imshow(segmentation)\n        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n        f.tight_layout()\n        f.savefig(savename)\n        plt.close()\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_35-85", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "    if mask_paths is None:\n        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n    masks_provided = mask_paths[0] != \"-1\"\n    if anomaly_scores is None:\n        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n\n    os.makedirs(savefolder, exist_ok=True)\n\n    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n        zip(image_paths, mask_paths, anomaly_scores, segmentations),\n        total=len(image_paths),\n        desc=\"Generating Segmentation Images...\",\n        leave=False,\n    ):\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = image_transform(image)\n        if not isinstance(image, np.ndarray):\n            image = image.numpy()\n\n        if masks_provided:\n            if mask_path is not None:\n                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n                mask = mask_transform(mask)\n                if not isinstance(mask, np.ndarray):\n                    mask = mask.numpy()\n            else:\n                mask = np.zeros_like(image)\n\n        savename = image_path.split(\"/\")\n        savename = \"_\".join(savename[-save_depth:])\n        savename = os.path.join(savefolder, savename)\n        f, axes = plt.subplots(1, 2 + int(masks_provided))\n        axes[0].imshow(image.transpose(1, 2, 0))\n        axes[1].imshow(mask.transpose(1, 2, 0))\n        axes[2].imshow(segmentation)\n        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n        f.tight_layout()\n        f.savefig(savename)\n        plt.close()\n\n\ndef create_storage_folder(\n    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n):\n    os.makedirs(main_folder_path, exist_ok=True)\n    project_path = os.path.join(main_folder_path, project_folder)\n    os.makedirs(project_path, exist_ok=True)\n    save_path = os.path.join(project_path, group_folder)\n    if mode == \"iterate\":\n        counter = 0", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_45-95", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "        total=len(image_paths),\n        desc=\"Generating Segmentation Images...\",\n        leave=False,\n    ):\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = image_transform(image)\n        if not isinstance(image, np.ndarray):\n            image = image.numpy()\n\n        if masks_provided:\n            if mask_path is not None:\n                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n                mask = mask_transform(mask)\n                if not isinstance(mask, np.ndarray):\n                    mask = mask.numpy()\n            else:\n                mask = np.zeros_like(image)\n\n        savename = image_path.split(\"/\")\n        savename = \"_\".join(savename[-save_depth:])\n        savename = os.path.join(savefolder, savename)\n        f, axes = plt.subplots(1, 2 + int(masks_provided))\n        axes[0].imshow(image.transpose(1, 2, 0))\n        axes[1].imshow(mask.transpose(1, 2, 0))\n        axes[2].imshow(segmentation)\n        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n        f.tight_layout()\n        f.savefig(savename)\n        plt.close()\n\n\ndef create_storage_folder(\n    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n):\n    os.makedirs(main_folder_path, exist_ok=True)\n    project_path = os.path.join(main_folder_path, project_folder)\n    os.makedirs(project_path, exist_ok=True)\n    save_path = os.path.join(project_path, group_folder)\n    if mode == \"iterate\":\n        counter = 0\n        while os.path.exists(save_path):\n            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n            counter += 1\n        os.makedirs(save_path)\n    elif mode == \"overwrite\":\n        os.makedirs(save_path, exist_ok=True)\n\n    return save_path\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_55-105", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "            if mask_path is not None:\n                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n                mask = mask_transform(mask)\n                if not isinstance(mask, np.ndarray):\n                    mask = mask.numpy()\n            else:\n                mask = np.zeros_like(image)\n\n        savename = image_path.split(\"/\")\n        savename = \"_\".join(savename[-save_depth:])\n        savename = os.path.join(savefolder, savename)\n        f, axes = plt.subplots(1, 2 + int(masks_provided))\n        axes[0].imshow(image.transpose(1, 2, 0))\n        axes[1].imshow(mask.transpose(1, 2, 0))\n        axes[2].imshow(segmentation)\n        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n        f.tight_layout()\n        f.savefig(savename)\n        plt.close()\n\n\ndef create_storage_folder(\n    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n):\n    os.makedirs(main_folder_path, exist_ok=True)\n    project_path = os.path.join(main_folder_path, project_folder)\n    os.makedirs(project_path, exist_ok=True)\n    save_path = os.path.join(project_path, group_folder)\n    if mode == \"iterate\":\n        counter = 0\n        while os.path.exists(save_path):\n            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n            counter += 1\n        os.makedirs(save_path)\n    elif mode == \"overwrite\":\n        os.makedirs(save_path, exist_ok=True)\n\n    return save_path\n\n\ndef set_torch_device(gpu_ids):\n    \"\"\"Returns correct torch.device.\n\n    Args:\n        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n    \"\"\"\n    if len(gpu_ids):\n        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n        return torch.device(\"cuda:{}\".format(gpu_ids[0]))", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_65-115", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "        savename = os.path.join(savefolder, savename)\n        f, axes = plt.subplots(1, 2 + int(masks_provided))\n        axes[0].imshow(image.transpose(1, 2, 0))\n        axes[1].imshow(mask.transpose(1, 2, 0))\n        axes[2].imshow(segmentation)\n        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n        f.tight_layout()\n        f.savefig(savename)\n        plt.close()\n\n\ndef create_storage_folder(\n    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n):\n    os.makedirs(main_folder_path, exist_ok=True)\n    project_path = os.path.join(main_folder_path, project_folder)\n    os.makedirs(project_path, exist_ok=True)\n    save_path = os.path.join(project_path, group_folder)\n    if mode == \"iterate\":\n        counter = 0\n        while os.path.exists(save_path):\n            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n            counter += 1\n        os.makedirs(save_path)\n    elif mode == \"overwrite\":\n        os.makedirs(save_path, exist_ok=True)\n\n    return save_path\n\n\ndef set_torch_device(gpu_ids):\n    \"\"\"Returns correct torch.device.\n\n    Args:\n        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n    \"\"\"\n    if len(gpu_ids):\n        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n        return torch.device(\"cuda:{}\".format(gpu_ids[0]))\n    return torch.device(\"cpu\")\n\n\ndef fix_seeds(seed, with_torch=True, with_cuda=True):\n    \"\"\"Fixed available seeds for reproducibility.\n\n    Args:\n        seed: [int] Seed value.\n        with_torch: Flag. If true, torch-related seeds are fixed.\n        with_cuda: Flag. If true, torch+cuda-related seeds are fixed", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_75-125", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "\ndef create_storage_folder(\n    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n):\n    os.makedirs(main_folder_path, exist_ok=True)\n    project_path = os.path.join(main_folder_path, project_folder)\n    os.makedirs(project_path, exist_ok=True)\n    save_path = os.path.join(project_path, group_folder)\n    if mode == \"iterate\":\n        counter = 0\n        while os.path.exists(save_path):\n            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n            counter += 1\n        os.makedirs(save_path)\n    elif mode == \"overwrite\":\n        os.makedirs(save_path, exist_ok=True)\n\n    return save_path\n\n\ndef set_torch_device(gpu_ids):\n    \"\"\"Returns correct torch.device.\n\n    Args:\n        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n    \"\"\"\n    if len(gpu_ids):\n        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n        return torch.device(\"cuda:{}\".format(gpu_ids[0]))\n    return torch.device(\"cpu\")\n\n\ndef fix_seeds(seed, with_torch=True, with_cuda=True):\n    \"\"\"Fixed available seeds for reproducibility.\n\n    Args:\n        seed: [int] Seed value.\n        with_torch: Flag. If true, torch-related seeds are fixed.\n        with_cuda: Flag. If true, torch+cuda-related seeds are fixed\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if with_torch:\n        torch.manual_seed(seed)\n    if with_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_85-135", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "        while os.path.exists(save_path):\n            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n            counter += 1\n        os.makedirs(save_path)\n    elif mode == \"overwrite\":\n        os.makedirs(save_path, exist_ok=True)\n\n    return save_path\n\n\ndef set_torch_device(gpu_ids):\n    \"\"\"Returns correct torch.device.\n\n    Args:\n        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n    \"\"\"\n    if len(gpu_ids):\n        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n        return torch.device(\"cuda:{}\".format(gpu_ids[0]))\n    return torch.device(\"cpu\")\n\n\ndef fix_seeds(seed, with_torch=True, with_cuda=True):\n    \"\"\"Fixed available seeds for reproducibility.\n\n    Args:\n        seed: [int] Seed value.\n        with_torch: Flag. If true, torch-related seeds are fixed.\n        with_cuda: Flag. If true, torch+cuda-related seeds are fixed\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if with_torch:\n        torch.manual_seed(seed)\n    if with_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef compute_and_store_final_results(\n    results_path,\n    results,\n    row_names=None,\n    column_names=[\n        \"Instance AUROC\",\n        \"Full Pixel AUROC\",\n        \"Full PRO\",\n        \"Anomaly Pixel AUROC\",", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_95-145", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "def set_torch_device(gpu_ids):\n    \"\"\"Returns correct torch.device.\n\n    Args:\n        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n    \"\"\"\n    if len(gpu_ids):\n        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n        return torch.device(\"cuda:{}\".format(gpu_ids[0]))\n    return torch.device(\"cpu\")\n\n\ndef fix_seeds(seed, with_torch=True, with_cuda=True):\n    \"\"\"Fixed available seeds for reproducibility.\n\n    Args:\n        seed: [int] Seed value.\n        with_torch: Flag. If true, torch-related seeds are fixed.\n        with_cuda: Flag. If true, torch+cuda-related seeds are fixed\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if with_torch:\n        torch.manual_seed(seed)\n    if with_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef compute_and_store_final_results(\n    results_path,\n    results,\n    row_names=None,\n    column_names=[\n        \"Instance AUROC\",\n        \"Full Pixel AUROC\",\n        \"Full PRO\",\n        \"Anomaly Pixel AUROC\",\n        \"Anomaly PRO\",\n    ],\n):\n    \"\"\"Store computed results as CSV file.\n\n    Args:\n        results_path: [str] Where to store result csv.\n        results: [List[List]] List of lists containing results per dataset,\n                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n                 [instance_auroc, full_pixelwisew_auroc, full_pro,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_105-155", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "    return torch.device(\"cpu\")\n\n\ndef fix_seeds(seed, with_torch=True, with_cuda=True):\n    \"\"\"Fixed available seeds for reproducibility.\n\n    Args:\n        seed: [int] Seed value.\n        with_torch: Flag. If true, torch-related seeds are fixed.\n        with_cuda: Flag. If true, torch+cuda-related seeds are fixed\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if with_torch:\n        torch.manual_seed(seed)\n    if with_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef compute_and_store_final_results(\n    results_path,\n    results,\n    row_names=None,\n    column_names=[\n        \"Instance AUROC\",\n        \"Full Pixel AUROC\",\n        \"Full PRO\",\n        \"Anomaly Pixel AUROC\",\n        \"Anomaly PRO\",\n    ],\n):\n    \"\"\"Store computed results as CSV file.\n\n    Args:\n        results_path: [str] Where to store result csv.\n        results: [List[List]] List of lists containing results per dataset,\n                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n                 [instance_auroc, full_pixelwisew_auroc, full_pro,\n                 anomaly-only_pw_auroc, anomaly-only_pro]\n    \"\"\"\n    if row_names is not None:\n        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n\n    mean_metrics = {}\n    for i, result_key in enumerate(column_names):\n        mean_metrics[result_key] = np.mean([x[i] for x in results])\n        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_115-165", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if with_torch:\n        torch.manual_seed(seed)\n    if with_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef compute_and_store_final_results(\n    results_path,\n    results,\n    row_names=None,\n    column_names=[\n        \"Instance AUROC\",\n        \"Full Pixel AUROC\",\n        \"Full PRO\",\n        \"Anomaly Pixel AUROC\",\n        \"Anomaly PRO\",\n    ],\n):\n    \"\"\"Store computed results as CSV file.\n\n    Args:\n        results_path: [str] Where to store result csv.\n        results: [List[List]] List of lists containing results per dataset,\n                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n                 [instance_auroc, full_pixelwisew_auroc, full_pro,\n                 anomaly-only_pw_auroc, anomaly-only_pro]\n    \"\"\"\n    if row_names is not None:\n        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n\n    mean_metrics = {}\n    for i, result_key in enumerate(column_names):\n        mean_metrics[result_key] = np.mean([x[i] for x in results])\n        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n\n    savename = os.path.join(results_path, \"results.csv\")\n    with open(savename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=\",\")\n        header = column_names\n        if row_names is not None:\n            header = [\"Row Names\"] + header\n\n        csv_writer.writerow(header)\n        for i, result_list in enumerate(results):\n            csv_row = result_list", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_125-175", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "\ndef compute_and_store_final_results(\n    results_path,\n    results,\n    row_names=None,\n    column_names=[\n        \"Instance AUROC\",\n        \"Full Pixel AUROC\",\n        \"Full PRO\",\n        \"Anomaly Pixel AUROC\",\n        \"Anomaly PRO\",\n    ],\n):\n    \"\"\"Store computed results as CSV file.\n\n    Args:\n        results_path: [str] Where to store result csv.\n        results: [List[List]] List of lists containing results per dataset,\n                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n                 [instance_auroc, full_pixelwisew_auroc, full_pro,\n                 anomaly-only_pw_auroc, anomaly-only_pro]\n    \"\"\"\n    if row_names is not None:\n        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n\n    mean_metrics = {}\n    for i, result_key in enumerate(column_names):\n        mean_metrics[result_key] = np.mean([x[i] for x in results])\n        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n\n    savename = os.path.join(results_path, \"results.csv\")\n    with open(savename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=\",\")\n        header = column_names\n        if row_names is not None:\n            header = [\"Row Names\"] + header\n\n        csv_writer.writerow(header)\n        for i, result_list in enumerate(results):\n            csv_row = result_list\n            if row_names is not None:\n                csv_row = [row_names[i]] + result_list\n            csv_writer.writerow(csv_row)\n        mean_scores = list(mean_metrics.values())\n        if row_names is not None:\n            mean_scores = [\"Mean\"] + mean_scores\n        csv_writer.writerow(mean_scores)\n\n    mean_metrics = {\"mean_{0}\".format(key): item for key, item in mean_metrics.items()}\n    return mean_metrics", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_135-175", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "        \"Anomaly PRO\",\n    ],\n):\n    \"\"\"Store computed results as CSV file.\n\n    Args:\n        results_path: [str] Where to store result csv.\n        results: [List[List]] List of lists containing results per dataset,\n                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n                 [instance_auroc, full_pixelwisew_auroc, full_pro,\n                 anomaly-only_pw_auroc, anomaly-only_pro]\n    \"\"\"\n    if row_names is not None:\n        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n\n    mean_metrics = {}\n    for i, result_key in enumerate(column_names):\n        mean_metrics[result_key] = np.mean([x[i] for x in results])\n        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n\n    savename = os.path.join(results_path, \"results.csv\")\n    with open(savename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=\",\")\n        header = column_names\n        if row_names is not None:\n            header = [\"Row Names\"] + header\n\n        csv_writer.writerow(header)\n        for i, result_list in enumerate(results):\n            csv_row = result_list\n            if row_names is not None:\n                csv_row = [row_names[i]] + result_list\n            csv_writer.writerow(csv_row)\n        mean_scores = list(mean_metrics.values())\n        if row_names is not None:\n            mean_scores = [\"Mean\"] + mean_scores\n        csv_writer.writerow(mean_scores)\n\n    mean_metrics = {\"mean_{0}\".format(key): item for key, item in mean_metrics.items()}\n    return mean_metrics", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-utils.py_145-175", "title": "amazon-science_patchcore-inspection-src-patchcore-utils.py", "text": "                 anomaly-only_pw_auroc, anomaly-only_pro]\n    \"\"\"\n    if row_names is not None:\n        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n\n    mean_metrics = {}\n    for i, result_key in enumerate(column_names):\n        mean_metrics[result_key] = np.mean([x[i] for x in results])\n        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n\n    savename = os.path.join(results_path, \"results.csv\")\n    with open(savename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=\",\")\n        header = column_names\n        if row_names is not None:\n            header = [\"Row Names\"] + header\n\n        csv_writer.writerow(header)\n        for i, result_list in enumerate(results):\n            csv_row = result_list\n            if row_names is not None:\n                csv_row = [row_names[i]] + result_list\n            csv_writer.writerow(csv_row)\n        mean_scores = list(mean_metrics.values())\n        if row_names is not None:\n            mean_scores = [\"Mean\"] + mean_scores\n        csv_writer.writerow(mean_scores)\n\n    mean_metrics = {\"mean_{0}\".format(key): item for key, item in mean_metrics.items()}\n    return mean_metrics", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "utils.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "utils.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_35-85", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_45-95", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_55-105", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_65-115", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_75-125", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_85-135", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_95-145", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_105-155", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_115-165", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_125-175", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_135-185", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_145-195", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_155-205", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_165-215", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_175-225", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_185-235", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "class RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_195-245", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_205-255", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_215-265", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_225-275", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_235-285", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_245-295", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_255-305", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_265-315", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_275-325", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_285-335", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_295-345", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "class NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_305-355", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_315-365", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n\n    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_325-375", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n\n    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_335-385", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n\n    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:\n        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n\n    def save_and_reset(self, save_folder: str) -> None:\n        self.save(save_folder)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_345-393", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n\n    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:\n        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n\n    def save_and_reset(self, save_folder: str) -> None:\n        self.save(save_folder)\n        self.nn_method.reset_index()\n\n    def load(self, load_folder: str, prepend: str = \"\") -> None:\n        self.nn_method.load(self._index_file(load_folder, prepend))\n        if os.path.exists(self._detection_file(load_folder, prepend)):\n            self.detection_features = self._load(\n                self._detection_file(load_folder, prepend)\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_355-393", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:\n        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n\n    def save_and_reset(self, save_folder: str) -> None:\n        self.save(save_folder)\n        self.nn_method.reset_index()\n\n    def load(self, load_folder: str, prepend: str = \"\") -> None:\n        self.nn_method.load(self._index_file(load_folder, prepend))\n        if os.path.exists(self._detection_file(load_folder, prepend)):\n            self.detection_features = self._load(\n                self._detection_file(load_folder, prepend)\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-common.py_365-393", "title": "amazon-science_patchcore-inspection-src-patchcore-common.py", "text": "\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:\n        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n\n    def save_and_reset(self, save_folder: str) -> None:\n        self.save(save_folder)\n        self.nn_method.reset_index()\n\n    def load(self, load_folder: str, prepend: str = \"\") -> None:\n        self.nn_method.load(self._index_file(load_folder, prepend))\n        if os.path.exists(self._detection_file(load_folder, prepend)):\n            self.detection_features = self._load(\n                self._detection_file(load_folder, prepend)\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "common.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 393, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "LOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_35-85", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_45-95", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_55-105", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_65-115", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_75-125", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_85-135", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_95-145", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_105-155", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_115-165", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_125-175", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_135-185", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_145-195", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_155-205", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_165-215", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_175-225", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_185-235", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_195-245", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_205-255", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_215-265", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_225-275", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_235-285", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_245-295", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_255-305", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_265-315", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n\n    def unpatch_scores(self, x, batchsize):\n        return x.reshape(batchsize, -1, *x.shape[1:])\n\n    def score(self, x):\n        was_numpy = False\n        if isinstance(x, np.ndarray):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_275-322", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n\n    def unpatch_scores(self, x, batchsize):\n        return x.reshape(batchsize, -1, *x.shape[1:])\n\n    def score(self, x):\n        was_numpy = False\n        if isinstance(x, np.ndarray):\n            was_numpy = True\n            x = torch.from_numpy(x)\n        while x.ndim > 1:\n            x = torch.max(x, dim=-1).values\n        if was_numpy:\n            return x.numpy()\n        return x", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_285-322", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n\n    def unpatch_scores(self, x, batchsize):\n        return x.reshape(batchsize, -1, *x.shape[1:])\n\n    def score(self, x):\n        was_numpy = False\n        if isinstance(x, np.ndarray):\n            was_numpy = True\n            x = torch.from_numpy(x)\n        while x.ndim > 1:\n            x = torch.max(x, dim=-1).values\n        if was_numpy:\n            return x.numpy()\n        return x", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-patchcore.py_295-322", "title": "amazon-science_patchcore-inspection-src-patchcore-patchcore.py", "text": "        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n\n    def unpatch_scores(self, x, batchsize):\n        return x.reshape(batchsize, -1, *x.shape[1:])\n\n    def score(self, x):\n        was_numpy = False\n        if isinstance(x, np.ndarray):\n            was_numpy = True\n            x = torch.from_numpy(x)\n        while x.ndim > 1:\n            x = torch.max(x, dim=-1).values\n        if was_numpy:\n            return x.numpy()\n        return x", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "patchcore.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 322, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "import timm  # noqa\nimport torchvision.models as models  # noqa\n\n_BACKBONES = {\n    \"alexnet\": \"models.alexnet(pretrained=True)\",\n    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n    '(pretrained=\"imagenet\", num_classes=1000)',\n    \"resnet50\": \"models.resnet50(pretrained=True)\",\n    \"resnet101\": \"models.resnet101(pretrained=True)\",\n    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "import timm  # noqa\nimport torchvision.models as models  # noqa\n\n_BACKBONES = {\n    \"alexnet\": \"models.alexnet(pretrained=True)\",\n    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n    '(pretrained=\"imagenet\", num_classes=1000)',\n    \"resnet50\": \"models.resnet50(pretrained=True)\",\n    \"resnet101\": \"models.resnet101(pretrained=True)\",\n    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "import timm  # noqa\nimport torchvision.models as models  # noqa\n\n_BACKBONES = {\n    \"alexnet\": \"models.alexnet(pretrained=True)\",\n    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n    '(pretrained=\"imagenet\", num_classes=1000)',\n    \"resnet50\": \"models.resnet50(pretrained=True)\",\n    \"resnet101\": \"models.resnet101(pretrained=True)\",\n    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch16_224\", pretrained=True)',\n    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True)',\n    \"vit_swin_base\": 'timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)',\n    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_5-51", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n    '(pretrained=\"imagenet\", num_classes=1000)',\n    \"resnet50\": \"models.resnet50(pretrained=True)\",\n    \"resnet101\": \"models.resnet101(pretrained=True)\",\n    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch16_224\", pretrained=True)',\n    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True)',\n    \"vit_swin_base\": 'timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)',\n    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',\n    \"efficientnet_b3a\": 'timm.create_model(\"efficientnet_b3a\", pretrained=True)',\n}\n\n\ndef load(name):\n    return eval(_BACKBONES[name])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_15-51", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch16_224\", pretrained=True)',\n    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True)',\n    \"vit_swin_base\": 'timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)',\n    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',\n    \"efficientnet_b3a\": 'timm.create_model(\"efficientnet_b3a\", pretrained=True)',\n}\n\n\ndef load(name):\n    return eval(_BACKBONES[name])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-backbones.py_25-51", "title": "amazon-science_patchcore-inspection-src-patchcore-backbones.py", "text": "    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch16_224\", pretrained=True)',\n    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True)',\n    \"vit_swin_base\": 'timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)',\n    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',\n    \"efficientnet_b3a\": 'timm.create_model(\"efficientnet_b3a\", pretrained=True)',\n}\n\n\ndef load(name):\n    return eval(_BACKBONES[name])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "backbones.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "backbones.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 51, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_0-25", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "import os\nfrom enum import Enum\n\nimport PIL\nimport torch\nfrom torchvision import transforms\n\n_CLASSNAMES = [\n    \"bottle\",\n    \"cable\",\n    \"capsule\",\n    \"carpet\",\n    \"grid\",\n    \"hazelnut\",\n    \"leather\",\n    \"metal_nut\",\n    \"pill\",\n    \"screw\",\n    \"tile\",\n    \"toothbrush\",\n    \"transistor\",\n    \"wood\",\n    \"zipper\",\n]\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_0-35", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "import os\nfrom enum import Enum\n\nimport PIL\nimport torch\nfrom torchvision import transforms\n\n_CLASSNAMES = [\n    \"bottle\",\n    \"cable\",\n    \"capsule\",\n    \"carpet\",\n    \"grid\",\n    \"hazelnut\",\n    \"leather\",\n    \"metal_nut\",\n    \"pill\",\n    \"screw\",\n    \"tile\",\n    \"toothbrush\",\n    \"transistor\",\n    \"wood\",\n    \"zipper\",\n]\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\nclass DatasetSplit(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_0-45", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "import os\nfrom enum import Enum\n\nimport PIL\nimport torch\nfrom torchvision import transforms\n\n_CLASSNAMES = [\n    \"bottle\",\n    \"cable\",\n    \"capsule\",\n    \"carpet\",\n    \"grid\",\n    \"hazelnut\",\n    \"leather\",\n    \"metal_nut\",\n    \"pill\",\n    \"screw\",\n    \"tile\",\n    \"toothbrush\",\n    \"transistor\",\n    \"wood\",\n    \"zipper\",\n]\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\nclass DatasetSplit(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n\nclass MVTecDataset(torch.utils.data.Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec.\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        classname,\n        resize=256,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_5-55", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "from torchvision import transforms\n\n_CLASSNAMES = [\n    \"bottle\",\n    \"cable\",\n    \"capsule\",\n    \"carpet\",\n    \"grid\",\n    \"hazelnut\",\n    \"leather\",\n    \"metal_nut\",\n    \"pill\",\n    \"screw\",\n    \"tile\",\n    \"toothbrush\",\n    \"transistor\",\n    \"wood\",\n    \"zipper\",\n]\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\nclass DatasetSplit(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n\nclass MVTecDataset(torch.utils.data.Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec.\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        classname,\n        resize=256,\n        imagesize=224,\n        split=DatasetSplit.TRAIN,\n        train_val_split=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: [str]. Path to the MVTec data folder.\n            classname: [str or None]. Name of MVTec class that should be\n                       provided in this dataset. If None, the datasets", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_15-65", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "    \"metal_nut\",\n    \"pill\",\n    \"screw\",\n    \"tile\",\n    \"toothbrush\",\n    \"transistor\",\n    \"wood\",\n    \"zipper\",\n]\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\nclass DatasetSplit(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n\nclass MVTecDataset(torch.utils.data.Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec.\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        classname,\n        resize=256,\n        imagesize=224,\n        split=DatasetSplit.TRAIN,\n        train_val_split=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: [str]. Path to the MVTec data folder.\n            classname: [str or None]. Name of MVTec class that should be\n                       provided in this dataset. If None, the datasets\n                       iterates over all available images.\n            resize: [int]. (Square) Size the loaded image initially gets\n                    resized to.\n            imagesize: [int]. (Square) Size the resized loaded image gets\n                       (center-)cropped to.\n            split: [enum-option]. Indicates if training or test split of the\n                   data should be used. Has to be an option taken from\n                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n                   mvtec.DatasetSplit.TEST will also load mask data.\n        \"\"\"", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_25-75", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "IMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\nclass DatasetSplit(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n\nclass MVTecDataset(torch.utils.data.Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec.\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        classname,\n        resize=256,\n        imagesize=224,\n        split=DatasetSplit.TRAIN,\n        train_val_split=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: [str]. Path to the MVTec data folder.\n            classname: [str or None]. Name of MVTec class that should be\n                       provided in this dataset. If None, the datasets\n                       iterates over all available images.\n            resize: [int]. (Square) Size the loaded image initially gets\n                    resized to.\n            imagesize: [int]. (Square) Size the resized loaded image gets\n                       (center-)cropped to.\n            split: [enum-option]. Indicates if training or test split of the\n                   data should be used. Has to be an option taken from\n                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n                   mvtec.DatasetSplit.TEST will also load mask data.\n        \"\"\"\n        super().__init__()\n        self.source = source\n        self.split = split\n        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n        self.train_val_split = train_val_split\n\n        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n\n        self.transform_img = [\n            transforms.Resize(resize),", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_35-85", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "class MVTecDataset(torch.utils.data.Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec.\n    \"\"\"\n\n    def __init__(\n        self,\n        source,\n        classname,\n        resize=256,\n        imagesize=224,\n        split=DatasetSplit.TRAIN,\n        train_val_split=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: [str]. Path to the MVTec data folder.\n            classname: [str or None]. Name of MVTec class that should be\n                       provided in this dataset. If None, the datasets\n                       iterates over all available images.\n            resize: [int]. (Square) Size the loaded image initially gets\n                    resized to.\n            imagesize: [int]. (Square) Size the resized loaded image gets\n                       (center-)cropped to.\n            split: [enum-option]. Indicates if training or test split of the\n                   data should be used. Has to be an option taken from\n                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n                   mvtec.DatasetSplit.TEST will also load mask data.\n        \"\"\"\n        super().__init__()\n        self.source = source\n        self.split = split\n        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n        self.train_val_split = train_val_split\n\n        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n\n        self.transform_img = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]\n        self.transform_img = transforms.Compose(self.transform_img)\n\n        self.transform_mask = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_45-95", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "        imagesize=224,\n        split=DatasetSplit.TRAIN,\n        train_val_split=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: [str]. Path to the MVTec data folder.\n            classname: [str or None]. Name of MVTec class that should be\n                       provided in this dataset. If None, the datasets\n                       iterates over all available images.\n            resize: [int]. (Square) Size the loaded image initially gets\n                    resized to.\n            imagesize: [int]. (Square) Size the resized loaded image gets\n                       (center-)cropped to.\n            split: [enum-option]. Indicates if training or test split of the\n                   data should be used. Has to be an option taken from\n                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n                   mvtec.DatasetSplit.TEST will also load mask data.\n        \"\"\"\n        super().__init__()\n        self.source = source\n        self.split = split\n        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n        self.train_val_split = train_val_split\n\n        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n\n        self.transform_img = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]\n        self.transform_img = transforms.Compose(self.transform_img)\n\n        self.transform_mask = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n        ]\n        self.transform_mask = transforms.Compose(self.transform_mask)\n\n        self.imagesize = (3, imagesize, imagesize)\n\n    def __getitem__(self, idx):\n        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = self.transform_img(image)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_55-105", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "                       iterates over all available images.\n            resize: [int]. (Square) Size the loaded image initially gets\n                    resized to.\n            imagesize: [int]. (Square) Size the resized loaded image gets\n                       (center-)cropped to.\n            split: [enum-option]. Indicates if training or test split of the\n                   data should be used. Has to be an option taken from\n                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n                   mvtec.DatasetSplit.TEST will also load mask data.\n        \"\"\"\n        super().__init__()\n        self.source = source\n        self.split = split\n        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n        self.train_val_split = train_val_split\n\n        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n\n        self.transform_img = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]\n        self.transform_img = transforms.Compose(self.transform_img)\n\n        self.transform_mask = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n        ]\n        self.transform_mask = transforms.Compose(self.transform_mask)\n\n        self.imagesize = (3, imagesize, imagesize)\n\n    def __getitem__(self, idx):\n        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = self.transform_img(image)\n\n        if self.split == DatasetSplit.TEST and mask_path is not None:\n            mask = PIL.Image.open(mask_path)\n            mask = self.transform_mask(mask)\n        else:\n            mask = torch.zeros([1, *image.size()[1:]])\n\n        return {\n            \"image\": image,\n            \"mask\": mask,\n            \"classname\": classname,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_65-115", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "        super().__init__()\n        self.source = source\n        self.split = split\n        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n        self.train_val_split = train_val_split\n\n        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n\n        self.transform_img = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]\n        self.transform_img = transforms.Compose(self.transform_img)\n\n        self.transform_mask = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n        ]\n        self.transform_mask = transforms.Compose(self.transform_mask)\n\n        self.imagesize = (3, imagesize, imagesize)\n\n    def __getitem__(self, idx):\n        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = self.transform_img(image)\n\n        if self.split == DatasetSplit.TEST and mask_path is not None:\n            mask = PIL.Image.open(mask_path)\n            mask = self.transform_mask(mask)\n        else:\n            mask = torch.zeros([1, *image.size()[1:]])\n\n        return {\n            \"image\": image,\n            \"mask\": mask,\n            \"classname\": classname,\n            \"anomaly\": anomaly,\n            \"is_anomaly\": int(anomaly != \"good\"),\n            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n            \"image_path\": image_path,\n        }\n\n    def __len__(self):\n        return len(self.data_to_iterate)\n\n    def get_image_data(self):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_75-125", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ]\n        self.transform_img = transforms.Compose(self.transform_img)\n\n        self.transform_mask = [\n            transforms.Resize(resize),\n            transforms.CenterCrop(imagesize),\n            transforms.ToTensor(),\n        ]\n        self.transform_mask = transforms.Compose(self.transform_mask)\n\n        self.imagesize = (3, imagesize, imagesize)\n\n    def __getitem__(self, idx):\n        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = self.transform_img(image)\n\n        if self.split == DatasetSplit.TEST and mask_path is not None:\n            mask = PIL.Image.open(mask_path)\n            mask = self.transform_mask(mask)\n        else:\n            mask = torch.zeros([1, *image.size()[1:]])\n\n        return {\n            \"image\": image,\n            \"mask\": mask,\n            \"classname\": classname,\n            \"anomaly\": anomaly,\n            \"is_anomaly\": int(anomaly != \"good\"),\n            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n            \"image_path\": image_path,\n        }\n\n    def __len__(self):\n        return len(self.data_to_iterate)\n\n    def get_image_data(self):\n        imgpaths_per_class = {}\n        maskpaths_per_class = {}\n\n        for classname in self.classnames_to_use:\n            classpath = os.path.join(self.source, classname, self.split.value)\n            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n            anomaly_types = os.listdir(classpath)\n\n            imgpaths_per_class[classname] = {}\n            maskpaths_per_class[classname] = {}", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_85-135", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "        ]\n        self.transform_mask = transforms.Compose(self.transform_mask)\n\n        self.imagesize = (3, imagesize, imagesize)\n\n    def __getitem__(self, idx):\n        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n        image = PIL.Image.open(image_path).convert(\"RGB\")\n        image = self.transform_img(image)\n\n        if self.split == DatasetSplit.TEST and mask_path is not None:\n            mask = PIL.Image.open(mask_path)\n            mask = self.transform_mask(mask)\n        else:\n            mask = torch.zeros([1, *image.size()[1:]])\n\n        return {\n            \"image\": image,\n            \"mask\": mask,\n            \"classname\": classname,\n            \"anomaly\": anomaly,\n            \"is_anomaly\": int(anomaly != \"good\"),\n            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n            \"image_path\": image_path,\n        }\n\n    def __len__(self):\n        return len(self.data_to_iterate)\n\n    def get_image_data(self):\n        imgpaths_per_class = {}\n        maskpaths_per_class = {}\n\n        for classname in self.classnames_to_use:\n            classpath = os.path.join(self.source, classname, self.split.value)\n            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n            anomaly_types = os.listdir(classpath)\n\n            imgpaths_per_class[classname] = {}\n            maskpaths_per_class[classname] = {}\n\n            for anomaly in anomaly_types:\n                anomaly_path = os.path.join(classpath, anomaly)\n                anomaly_files = sorted(os.listdir(anomaly_path))\n                imgpaths_per_class[classname][anomaly] = [\n                    os.path.join(anomaly_path, x) for x in anomaly_files\n                ]\n\n                if self.train_val_split < 1.0:\n                    n_images = len(imgpaths_per_class[classname][anomaly])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_95-145", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "        if self.split == DatasetSplit.TEST and mask_path is not None:\n            mask = PIL.Image.open(mask_path)\n            mask = self.transform_mask(mask)\n        else:\n            mask = torch.zeros([1, *image.size()[1:]])\n\n        return {\n            \"image\": image,\n            \"mask\": mask,\n            \"classname\": classname,\n            \"anomaly\": anomaly,\n            \"is_anomaly\": int(anomaly != \"good\"),\n            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n            \"image_path\": image_path,\n        }\n\n    def __len__(self):\n        return len(self.data_to_iterate)\n\n    def get_image_data(self):\n        imgpaths_per_class = {}\n        maskpaths_per_class = {}\n\n        for classname in self.classnames_to_use:\n            classpath = os.path.join(self.source, classname, self.split.value)\n            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n            anomaly_types = os.listdir(classpath)\n\n            imgpaths_per_class[classname] = {}\n            maskpaths_per_class[classname] = {}\n\n            for anomaly in anomaly_types:\n                anomaly_path = os.path.join(classpath, anomaly)\n                anomaly_files = sorted(os.listdir(anomaly_path))\n                imgpaths_per_class[classname][anomaly] = [\n                    os.path.join(anomaly_path, x) for x in anomaly_files\n                ]\n\n                if self.train_val_split < 1.0:\n                    n_images = len(imgpaths_per_class[classname][anomaly])\n                    train_val_split_idx = int(n_images * self.train_val_split)\n                    if self.split == DatasetSplit.TRAIN:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][:train_val_split_idx]\n                    elif self.split == DatasetSplit.VAL:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][train_val_split_idx:]\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_105-155", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "            \"anomaly\": anomaly,\n            \"is_anomaly\": int(anomaly != \"good\"),\n            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n            \"image_path\": image_path,\n        }\n\n    def __len__(self):\n        return len(self.data_to_iterate)\n\n    def get_image_data(self):\n        imgpaths_per_class = {}\n        maskpaths_per_class = {}\n\n        for classname in self.classnames_to_use:\n            classpath = os.path.join(self.source, classname, self.split.value)\n            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n            anomaly_types = os.listdir(classpath)\n\n            imgpaths_per_class[classname] = {}\n            maskpaths_per_class[classname] = {}\n\n            for anomaly in anomaly_types:\n                anomaly_path = os.path.join(classpath, anomaly)\n                anomaly_files = sorted(os.listdir(anomaly_path))\n                imgpaths_per_class[classname][anomaly] = [\n                    os.path.join(anomaly_path, x) for x in anomaly_files\n                ]\n\n                if self.train_val_split < 1.0:\n                    n_images = len(imgpaths_per_class[classname][anomaly])\n                    train_val_split_idx = int(n_images * self.train_val_split)\n                    if self.split == DatasetSplit.TRAIN:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][:train_val_split_idx]\n                    elif self.split == DatasetSplit.VAL:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][train_val_split_idx:]\n\n                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n                    maskpaths_per_class[classname][anomaly] = [\n                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n                    ]\n                else:\n                    maskpaths_per_class[classname][\"good\"] = None\n\n        # Unrolls the data dictionary to an easy-to-iterate list.", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_115-165", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "        imgpaths_per_class = {}\n        maskpaths_per_class = {}\n\n        for classname in self.classnames_to_use:\n            classpath = os.path.join(self.source, classname, self.split.value)\n            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n            anomaly_types = os.listdir(classpath)\n\n            imgpaths_per_class[classname] = {}\n            maskpaths_per_class[classname] = {}\n\n            for anomaly in anomaly_types:\n                anomaly_path = os.path.join(classpath, anomaly)\n                anomaly_files = sorted(os.listdir(anomaly_path))\n                imgpaths_per_class[classname][anomaly] = [\n                    os.path.join(anomaly_path, x) for x in anomaly_files\n                ]\n\n                if self.train_val_split < 1.0:\n                    n_images = len(imgpaths_per_class[classname][anomaly])\n                    train_val_split_idx = int(n_images * self.train_val_split)\n                    if self.split == DatasetSplit.TRAIN:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][:train_val_split_idx]\n                    elif self.split == DatasetSplit.VAL:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][train_val_split_idx:]\n\n                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n                    maskpaths_per_class[classname][anomaly] = [\n                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n                    ]\n                else:\n                    maskpaths_per_class[classname][\"good\"] = None\n\n        # Unrolls the data dictionary to an easy-to-iterate list.\n        data_to_iterate = []\n        for classname in sorted(imgpaths_per_class.keys()):\n            for anomaly in sorted(imgpaths_per_class[classname].keys()):\n                for i, image_path in enumerate(imgpaths_per_class[classname][anomaly]):\n                    data_tuple = [classname, anomaly, image_path]\n                    if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                        data_tuple.append(maskpaths_per_class[classname][anomaly][i])\n                    else:\n                        data_tuple.append(None)\n                    data_to_iterate.append(data_tuple)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_125-167", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "\n            for anomaly in anomaly_types:\n                anomaly_path = os.path.join(classpath, anomaly)\n                anomaly_files = sorted(os.listdir(anomaly_path))\n                imgpaths_per_class[classname][anomaly] = [\n                    os.path.join(anomaly_path, x) for x in anomaly_files\n                ]\n\n                if self.train_val_split < 1.0:\n                    n_images = len(imgpaths_per_class[classname][anomaly])\n                    train_val_split_idx = int(n_images * self.train_val_split)\n                    if self.split == DatasetSplit.TRAIN:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][:train_val_split_idx]\n                    elif self.split == DatasetSplit.VAL:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][train_val_split_idx:]\n\n                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n                    maskpaths_per_class[classname][anomaly] = [\n                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n                    ]\n                else:\n                    maskpaths_per_class[classname][\"good\"] = None\n\n        # Unrolls the data dictionary to an easy-to-iterate list.\n        data_to_iterate = []\n        for classname in sorted(imgpaths_per_class.keys()):\n            for anomaly in sorted(imgpaths_per_class[classname].keys()):\n                for i, image_path in enumerate(imgpaths_per_class[classname][anomaly]):\n                    data_tuple = [classname, anomaly, image_path]\n                    if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                        data_tuple.append(maskpaths_per_class[classname][anomaly][i])\n                    else:\n                        data_tuple.append(None)\n                    data_to_iterate.append(data_tuple)\n\n        return imgpaths_per_class, data_to_iterate", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 167, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 167, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py_135-167", "title": "amazon-science_patchcore-inspection-src-patchcore-datasets-mvtec.py", "text": "                    train_val_split_idx = int(n_images * self.train_val_split)\n                    if self.split == DatasetSplit.TRAIN:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][:train_val_split_idx]\n                    elif self.split == DatasetSplit.VAL:\n                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n                            classname\n                        ][anomaly][train_val_split_idx:]\n\n                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n                    maskpaths_per_class[classname][anomaly] = [\n                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n                    ]\n                else:\n                    maskpaths_per_class[classname][\"good\"] = None\n\n        # Unrolls the data dictionary to an easy-to-iterate list.\n        data_to_iterate = []\n        for classname in sorted(imgpaths_per_class.keys()):\n            for anomaly in sorted(imgpaths_per_class[classname].keys()):\n                for i, image_path in enumerate(imgpaths_per_class[classname][anomaly]):\n                    data_tuple = [classname, anomaly, image_path]\n                    if self.split == DatasetSplit.TEST and anomaly != \"good\":\n                        data_tuple.append(maskpaths_per_class[classname][anomaly][i])\n                    else:\n                        data_tuple.append(None)\n                    data_to_iterate.append(data_tuple)\n\n        return imgpaths_per_class, data_to_iterate", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "datasets", "mvtec.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 167, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}, {"fpath_tuple": ["amazon-science_patchcore-inspection", "build", "lib", "patchcore", "datasets", "mvtec.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 167, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_0-25", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "import contextlib\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_0-35", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "import contextlib\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--log_group\", type=str, default=\"group\")\n@click.option(\"--log_project\", type=str, default=\"project\")\n@click.option(\"--save_segmentation_images\", is_flag=True)\n@click.option(\"--save_patchcore_model\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_0-45", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "import contextlib\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--log_group\", type=str, default=\"group\")\n@click.option(\"--log_project\", type=str, default=\"project\")\n@click.option(\"--save_segmentation_images\", is_flag=True)\n@click.option(\"--save_patchcore_model\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(\n    methods,\n    results_path,\n    gpu,\n    seed,\n    log_group,\n    log_project,\n    save_segmentation_images,\n    save_patchcore_model,\n):\n    methods = {key: item for (key, item) in methods}", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_5-55", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "import click\nimport numpy as np\nimport torch\n\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--log_group\", type=str, default=\"group\")\n@click.option(\"--log_project\", type=str, default=\"project\")\n@click.option(\"--save_segmentation_images\", is_flag=True)\n@click.option(\"--save_patchcore_model\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(\n    methods,\n    results_path,\n    gpu,\n    seed,\n    log_group,\n    log_project,\n    save_segmentation_images,\n    save_patchcore_model,\n):\n    methods = {key: item for (key, item) in methods}\n\n    run_save_path = patchcore.utils.create_storage_folder(\n        results_path, log_project, log_group, mode=\"iterate\"\n    )\n\n    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_15-65", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--log_group\", type=str, default=\"group\")\n@click.option(\"--log_project\", type=str, default=\"project\")\n@click.option(\"--save_segmentation_images\", is_flag=True)\n@click.option(\"--save_patchcore_model\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(\n    methods,\n    results_path,\n    gpu,\n    seed,\n    log_group,\n    log_project,\n    save_segmentation_images,\n    save_patchcore_model,\n):\n    methods = {key: item for (key, item) in methods}\n\n    run_save_path = patchcore.utils.create_storage_folder(\n        results_path, log_project, log_group, mode=\"iterate\"\n    )\n\n    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_25-75", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "@click.option(\"--log_group\", type=str, default=\"group\")\n@click.option(\"--log_project\", type=str, default=\"project\")\n@click.option(\"--save_segmentation_images\", is_flag=True)\n@click.option(\"--save_patchcore_model\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(\n    methods,\n    results_path,\n    gpu,\n    seed,\n    log_group,\n    log_project,\n    save_segmentation_images,\n    save_patchcore_model,\n):\n    methods = {key: item for (key, item) in methods}\n\n    run_save_path = patchcore.utils.create_storage_folder(\n        results_path, log_project, log_group, mode=\"iterate\"\n    )\n\n    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"training\"].name,\n                dataloader_count + 1,\n                len(list_of_dataloaders),\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_35-85", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "    methods,\n    results_path,\n    gpu,\n    seed,\n    log_group,\n    log_project,\n    save_segmentation_images,\n    save_patchcore_model,\n):\n    methods = {key: item for (key, item) in methods}\n\n    run_save_path = patchcore.utils.create_storage_folder(\n        results_path, log_project, log_group, mode=\"iterate\"\n    )\n\n    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"training\"].name,\n                dataloader_count + 1,\n                len(list_of_dataloaders),\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"training\"].name\n\n        with device_context:\n            torch.cuda.empty_cache()\n            imagesize = dataloaders[\"training\"].dataset.imagesize\n            sampler = methods[\"get_sampler\"](\n                device,\n            )\n            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n            if len(PatchCore_list) > 1:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_45-95", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n    run_save_path = patchcore.utils.create_storage_folder(\n        results_path, log_project, log_group, mode=\"iterate\"\n    )\n\n    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"training\"].name,\n                dataloader_count + 1,\n                len(list_of_dataloaders),\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"training\"].name\n\n        with device_context:\n            torch.cuda.empty_cache()\n            imagesize = dataloaders[\"training\"].dataset.imagesize\n            sampler = methods[\"get_sampler\"](\n                device,\n            )\n            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n            if len(PatchCore_list) > 1:\n                LOGGER.info(\n                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n                )\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                if PatchCore.backbone.seed is not None:\n                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n                LOGGER.info(\n                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n                )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_55-105", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"training\"].name,\n                dataloader_count + 1,\n                len(list_of_dataloaders),\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"training\"].name\n\n        with device_context:\n            torch.cuda.empty_cache()\n            imagesize = dataloaders[\"training\"].dataset.imagesize\n            sampler = methods[\"get_sampler\"](\n                device,\n            )\n            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n            if len(PatchCore_list) > 1:\n                LOGGER.info(\n                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n                )\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                if PatchCore.backbone.seed is not None:\n                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n                LOGGER.info(\n                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n                )\n                torch.cuda.empty_cache()\n                PatchCore.fit(dataloaders[\"training\"])\n\n            torch.cuda.empty_cache()\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_65-115", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"training\"].name,\n                dataloader_count + 1,\n                len(list_of_dataloaders),\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"training\"].name\n\n        with device_context:\n            torch.cuda.empty_cache()\n            imagesize = dataloaders[\"training\"].dataset.imagesize\n            sampler = methods[\"get_sampler\"](\n                device,\n            )\n            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n            if len(PatchCore_list) > 1:\n                LOGGER.info(\n                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n                )\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                if PatchCore.backbone.seed is not None:\n                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n                LOGGER.info(\n                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n                )\n                torch.cuda.empty_cache()\n                PatchCore.fit(dataloaders[\"training\"])\n\n            torch.cuda.empty_cache()\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_75-125", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        dataset_name = dataloaders[\"training\"].name\n\n        with device_context:\n            torch.cuda.empty_cache()\n            imagesize = dataloaders[\"training\"].dataset.imagesize\n            sampler = methods[\"get_sampler\"](\n                device,\n            )\n            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n            if len(PatchCore_list) > 1:\n                LOGGER.info(\n                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n                )\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                if PatchCore.backbone.seed is not None:\n                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n                LOGGER.info(\n                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n                )\n                torch.cuda.empty_cache()\n                PatchCore.fit(dataloaders[\"training\"])\n\n            torch.cuda.empty_cache()\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_85-135", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                LOGGER.info(\n                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n                )\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                if PatchCore.backbone.seed is not None:\n                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n                LOGGER.info(\n                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n                )\n                torch.cuda.empty_cache()\n                PatchCore.fit(dataloaders[\"training\"])\n\n            torch.cuda.empty_cache()\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_95-145", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                torch.cuda.empty_cache()\n                PatchCore.fit(dataloaders[\"training\"])\n\n            torch.cuda.empty_cache()\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # (Optional) Plot example images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_105-155", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # (Optional) Plot example images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_115-165", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # (Optional) Plot example images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                image_save_path = os.path.join(\n                    run_save_path, \"segmentation_images\", dataset_name\n                )\n                os.makedirs(image_save_path, exist_ok=True)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_125-175", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # (Optional) Plot example images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                image_save_path = os.path.join(\n                    run_save_path, \"segmentation_images\", dataset_name\n                )\n                os.makedirs(image_save_path, exist_ok=True)\n                patchcore.utils.plot_segmentation_images(\n                    image_save_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_135-185", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            ]\n\n            # (Optional) Plot example images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                image_save_path = os.path.join(\n                    run_save_path, \"segmentation_images\", dataset_name\n                )\n                os.makedirs(image_save_path, exist_ok=True)\n                patchcore.utils.plot_segmentation_images(\n                    image_save_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_145-195", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                image_save_path = os.path.join(\n                    run_save_path, \"segmentation_images\", dataset_name\n                )\n                os.makedirs(image_save_path, exist_ok=True)\n                patchcore.utils.plot_segmentation_images(\n                    image_save_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs],\n                [masks_gt[i] for i in sel_idxs],\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_155-205", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                image_save_path = os.path.join(\n                    run_save_path, \"segmentation_images\", dataset_name\n                )\n                os.makedirs(image_save_path, exist_ok=True)\n                patchcore.utils.plot_segmentation_images(\n                    image_save_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs],\n                [masks_gt[i] for i in sel_idxs],\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_165-215", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                patchcore.utils.plot_segmentation_images(\n                    image_save_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs],\n                [masks_gt[i] for i in sel_idxs],\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            # (Optional) Store PatchCore model for later re-use.\n            # SAVE all patchcores only if mean_threshold is passed?\n            if save_patchcore_model:\n                patchcore_save_path = os.path.join(\n                    run_save_path, \"models\", dataset_name", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_175-225", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            LOGGER.info(\"Computing evaluation metrics.\")\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs],\n                [masks_gt[i] for i in sel_idxs],\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            # (Optional) Store PatchCore model for later re-use.\n            # SAVE all patchcores only if mean_threshold is passed?\n            if save_patchcore_model:\n                patchcore_save_path = os.path.join(\n                    run_save_path, \"models\", dataset_name\n                )\n                os.makedirs(patchcore_save_path, exist_ok=True)\n                for i, PatchCore in enumerate(PatchCore_list):\n                    prepend = (\n                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n                        if len(PatchCore_list) > 1\n                        else \"\"\n                    )\n                    PatchCore.save_to_path(patchcore_save_path, prepend)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_185-235", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n            # Compute PRO score & PW Auroc only images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs],\n                [masks_gt[i] for i in sel_idxs],\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            # (Optional) Store PatchCore model for later re-use.\n            # SAVE all patchcores only if mean_threshold is passed?\n            if save_patchcore_model:\n                patchcore_save_path = os.path.join(\n                    run_save_path, \"models\", dataset_name\n                )\n                os.makedirs(patchcore_save_path, exist_ok=True)\n                for i, PatchCore in enumerate(PatchCore_list):\n                    prepend = (\n                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n                        if len(PatchCore_list) > 1\n                        else \"\"\n                    )\n                    PatchCore.save_to_path(patchcore_save_path, prepend)\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    # Store all results and mean scores to a csv-file.\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        run_save_path,\n        result_scores,\n        column_names=result_metric_names,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_195-245", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            # (Optional) Store PatchCore model for later re-use.\n            # SAVE all patchcores only if mean_threshold is passed?\n            if save_patchcore_model:\n                patchcore_save_path = os.path.join(\n                    run_save_path, \"models\", dataset_name\n                )\n                os.makedirs(patchcore_save_path, exist_ok=True)\n                for i, PatchCore in enumerate(PatchCore_list):\n                    prepend = (\n                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n                        if len(PatchCore_list) > 1\n                        else \"\"\n                    )\n                    PatchCore.save_to_path(patchcore_save_path, prepend)\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    # Store all results and mean scores to a csv-file.\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        run_save_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core\")\n# Pretraining-specific parameters.\n@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n# Parameters for Glue-code (to merge different parts of the pipeline.\n@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_205-255", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            # (Optional) Store PatchCore model for later re-use.\n            # SAVE all patchcores only if mean_threshold is passed?\n            if save_patchcore_model:\n                patchcore_save_path = os.path.join(\n                    run_save_path, \"models\", dataset_name\n                )\n                os.makedirs(patchcore_save_path, exist_ok=True)\n                for i, PatchCore in enumerate(PatchCore_list):\n                    prepend = (\n                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n                        if len(PatchCore_list) > 1\n                        else \"\"\n                    )\n                    PatchCore.save_to_path(patchcore_save_path, prepend)\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    # Store all results and mean scores to a csv-file.\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        run_save_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core\")\n# Pretraining-specific parameters.\n@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n# Parameters for Glue-code (to merge different parts of the pipeline.\n@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n@click.option(\"--target_embed_dimension\", type=int, default=1024)\n@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n# Nearest-Neighbour Anomaly Scorer parameters.\n@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n# Patch-parameters.\n@click.option(\"--patchsize\", type=int, default=3)\n@click.option(\"--patchscore\", type=str, default=\"max\")\n@click.option(\"--patchoverlap\", type=float, default=0.0)\n@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_215-265", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                )\n                os.makedirs(patchcore_save_path, exist_ok=True)\n                for i, PatchCore in enumerate(PatchCore_list):\n                    prepend = (\n                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n                        if len(PatchCore_list) > 1\n                        else \"\"\n                    )\n                    PatchCore.save_to_path(patchcore_save_path, prepend)\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    # Store all results and mean scores to a csv-file.\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        run_save_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core\")\n# Pretraining-specific parameters.\n@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n# Parameters for Glue-code (to merge different parts of the pipeline.\n@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n@click.option(\"--target_embed_dimension\", type=int, default=1024)\n@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n# Nearest-Neighbour Anomaly Scorer parameters.\n@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n# Patch-parameters.\n@click.option(\"--patchsize\", type=int, default=3)\n@click.option(\"--patchscore\", type=str, default=\"max\")\n@click.option(\"--patchoverlap\", type=float, default=0.0)\n@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core(\n    backbone_names,\n    layers_to_extract_from,\n    pretrain_embed_dimension,\n    target_embed_dimension,\n    preprocessing,\n    aggregation,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_225-275", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        LOGGER.info(\"\\n\\n-----\\n\")\n\n    # Store all results and mean scores to a csv-file.\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        run_save_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core\")\n# Pretraining-specific parameters.\n@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n# Parameters for Glue-code (to merge different parts of the pipeline.\n@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n@click.option(\"--target_embed_dimension\", type=int, default=1024)\n@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n# Nearest-Neighbour Anomaly Scorer parameters.\n@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n# Patch-parameters.\n@click.option(\"--patchsize\", type=int, default=3)\n@click.option(\"--patchscore\", type=str, default=\"max\")\n@click.option(\"--patchoverlap\", type=float, default=0.0)\n@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core(\n    backbone_names,\n    layers_to_extract_from,\n    pretrain_embed_dimension,\n    target_embed_dimension,\n    preprocessing,\n    aggregation,\n    patchsize,\n    patchscore,\n    patchoverlap,\n    anomaly_scorer_num_nn,\n    patchsize_aggregate,\n    faiss_on_gpu,\n    faiss_num_workers,\n):\n    backbone_names = list(backbone_names)\n    if len(backbone_names) > 1:", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_235-285", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core\")\n# Pretraining-specific parameters.\n@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n# Parameters for Glue-code (to merge different parts of the pipeline.\n@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n@click.option(\"--target_embed_dimension\", type=int, default=1024)\n@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n# Nearest-Neighbour Anomaly Scorer parameters.\n@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n# Patch-parameters.\n@click.option(\"--patchsize\", type=int, default=3)\n@click.option(\"--patchscore\", type=str, default=\"max\")\n@click.option(\"--patchoverlap\", type=float, default=0.0)\n@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core(\n    backbone_names,\n    layers_to_extract_from,\n    pretrain_embed_dimension,\n    target_embed_dimension,\n    preprocessing,\n    aggregation,\n    patchsize,\n    patchscore,\n    patchoverlap,\n    anomaly_scorer_num_nn,\n    patchsize_aggregate,\n    faiss_on_gpu,\n    faiss_num_workers,\n):\n    backbone_names = list(backbone_names)\n    if len(backbone_names) > 1:\n        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n        for layer in layers_to_extract_from:\n            idx = int(layer.split(\".\")[0])\n            layer = \".\".join(layer.split(\".\")[1:])\n            layers_to_extract_from_coll[idx].append(layer)\n    else:\n        layers_to_extract_from_coll = [layers_to_extract_from]\n\n    def get_patchcore(input_shape, sampler, device):\n        loaded_patchcores = []", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_245-295", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "@click.option(\"--target_embed_dimension\", type=int, default=1024)\n@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n# Nearest-Neighbour Anomaly Scorer parameters.\n@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n# Patch-parameters.\n@click.option(\"--patchsize\", type=int, default=3)\n@click.option(\"--patchscore\", type=str, default=\"max\")\n@click.option(\"--patchoverlap\", type=float, default=0.0)\n@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core(\n    backbone_names,\n    layers_to_extract_from,\n    pretrain_embed_dimension,\n    target_embed_dimension,\n    preprocessing,\n    aggregation,\n    patchsize,\n    patchscore,\n    patchoverlap,\n    anomaly_scorer_num_nn,\n    patchsize_aggregate,\n    faiss_on_gpu,\n    faiss_num_workers,\n):\n    backbone_names = list(backbone_names)\n    if len(backbone_names) > 1:\n        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n        for layer in layers_to_extract_from:\n            idx = int(layer.split(\".\")[0])\n            layer = \".\".join(layer.split(\".\")[1:])\n            layers_to_extract_from_coll[idx].append(layer)\n    else:\n        layers_to_extract_from_coll = [layers_to_extract_from]\n\n    def get_patchcore(input_shape, sampler, device):\n        loaded_patchcores = []\n        for backbone_name, layers_to_extract_from in zip(\n            backbone_names, layers_to_extract_from_coll\n        ):\n            backbone_seed = None\n            if \".seed-\" in backbone_name:\n                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n                    backbone_name.split(\"-\")[-1]\n                )\n            backbone = patchcore.backbones.load(backbone_name)\n            backbone.name, backbone.seed = backbone_name, backbone_seed", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_255-305", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core(\n    backbone_names,\n    layers_to_extract_from,\n    pretrain_embed_dimension,\n    target_embed_dimension,\n    preprocessing,\n    aggregation,\n    patchsize,\n    patchscore,\n    patchoverlap,\n    anomaly_scorer_num_nn,\n    patchsize_aggregate,\n    faiss_on_gpu,\n    faiss_num_workers,\n):\n    backbone_names = list(backbone_names)\n    if len(backbone_names) > 1:\n        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n        for layer in layers_to_extract_from:\n            idx = int(layer.split(\".\")[0])\n            layer = \".\".join(layer.split(\".\")[1:])\n            layers_to_extract_from_coll[idx].append(layer)\n    else:\n        layers_to_extract_from_coll = [layers_to_extract_from]\n\n    def get_patchcore(input_shape, sampler, device):\n        loaded_patchcores = []\n        for backbone_name, layers_to_extract_from in zip(\n            backbone_names, layers_to_extract_from_coll\n        ):\n            backbone_seed = None\n            if \".seed-\" in backbone_name:\n                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n                    backbone_name.split(\"-\")[-1]\n                )\n            backbone = patchcore.backbones.load(backbone_name)\n            backbone.name, backbone.seed = backbone_name, backbone_seed\n\n            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n\n            patchcore_instance = patchcore.patchcore.PatchCore(device)\n            patchcore_instance.load(\n                backbone=backbone,\n                layers_to_extract_from=layers_to_extract_from,\n                device=device,\n                input_shape=input_shape,\n                pretrain_embed_dimension=pretrain_embed_dimension,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_265-315", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "    patchsize,\n    patchscore,\n    patchoverlap,\n    anomaly_scorer_num_nn,\n    patchsize_aggregate,\n    faiss_on_gpu,\n    faiss_num_workers,\n):\n    backbone_names = list(backbone_names)\n    if len(backbone_names) > 1:\n        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n        for layer in layers_to_extract_from:\n            idx = int(layer.split(\".\")[0])\n            layer = \".\".join(layer.split(\".\")[1:])\n            layers_to_extract_from_coll[idx].append(layer)\n    else:\n        layers_to_extract_from_coll = [layers_to_extract_from]\n\n    def get_patchcore(input_shape, sampler, device):\n        loaded_patchcores = []\n        for backbone_name, layers_to_extract_from in zip(\n            backbone_names, layers_to_extract_from_coll\n        ):\n            backbone_seed = None\n            if \".seed-\" in backbone_name:\n                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n                    backbone_name.split(\"-\")[-1]\n                )\n            backbone = patchcore.backbones.load(backbone_name)\n            backbone.name, backbone.seed = backbone_name, backbone_seed\n\n            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n\n            patchcore_instance = patchcore.patchcore.PatchCore(device)\n            patchcore_instance.load(\n                backbone=backbone,\n                layers_to_extract_from=layers_to_extract_from,\n                device=device,\n                input_shape=input_shape,\n                pretrain_embed_dimension=pretrain_embed_dimension,\n                target_embed_dimension=target_embed_dimension,\n                patchsize=patchsize,\n                featuresampler=sampler,\n                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n                nn_method=nn_method,\n            )\n            loaded_patchcores.append(patchcore_instance)\n        return loaded_patchcores\n\n    return (\"get_patchcore\", get_patchcore)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_275-325", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n        for layer in layers_to_extract_from:\n            idx = int(layer.split(\".\")[0])\n            layer = \".\".join(layer.split(\".\")[1:])\n            layers_to_extract_from_coll[idx].append(layer)\n    else:\n        layers_to_extract_from_coll = [layers_to_extract_from]\n\n    def get_patchcore(input_shape, sampler, device):\n        loaded_patchcores = []\n        for backbone_name, layers_to_extract_from in zip(\n            backbone_names, layers_to_extract_from_coll\n        ):\n            backbone_seed = None\n            if \".seed-\" in backbone_name:\n                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n                    backbone_name.split(\"-\")[-1]\n                )\n            backbone = patchcore.backbones.load(backbone_name)\n            backbone.name, backbone.seed = backbone_name, backbone_seed\n\n            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n\n            patchcore_instance = patchcore.patchcore.PatchCore(device)\n            patchcore_instance.load(\n                backbone=backbone,\n                layers_to_extract_from=layers_to_extract_from,\n                device=device,\n                input_shape=input_shape,\n                pretrain_embed_dimension=pretrain_embed_dimension,\n                target_embed_dimension=target_embed_dimension,\n                patchsize=patchsize,\n                featuresampler=sampler,\n                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n                nn_method=nn_method,\n            )\n            loaded_patchcores.append(patchcore_instance)\n        return loaded_patchcores\n\n    return (\"get_patchcore\", get_patchcore)\n\n\n@main.command(\"sampler\")\n@click.argument(\"name\", type=str)\n@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\ndef sampler(name, percentage):\n    def get_sampler(device):\n        if name == \"identity\":\n            return patchcore.sampler.IdentitySampler()\n        elif name == \"greedy_coreset\":", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_285-335", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "        for backbone_name, layers_to_extract_from in zip(\n            backbone_names, layers_to_extract_from_coll\n        ):\n            backbone_seed = None\n            if \".seed-\" in backbone_name:\n                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n                    backbone_name.split(\"-\")[-1]\n                )\n            backbone = patchcore.backbones.load(backbone_name)\n            backbone.name, backbone.seed = backbone_name, backbone_seed\n\n            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n\n            patchcore_instance = patchcore.patchcore.PatchCore(device)\n            patchcore_instance.load(\n                backbone=backbone,\n                layers_to_extract_from=layers_to_extract_from,\n                device=device,\n                input_shape=input_shape,\n                pretrain_embed_dimension=pretrain_embed_dimension,\n                target_embed_dimension=target_embed_dimension,\n                patchsize=patchsize,\n                featuresampler=sampler,\n                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n                nn_method=nn_method,\n            )\n            loaded_patchcores.append(patchcore_instance)\n        return loaded_patchcores\n\n    return (\"get_patchcore\", get_patchcore)\n\n\n@main.command(\"sampler\")\n@click.argument(\"name\", type=str)\n@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\ndef sampler(name, percentage):\n    def get_sampler(device):\n        if name == \"identity\":\n            return patchcore.sampler.IdentitySampler()\n        elif name == \"greedy_coreset\":\n            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n        elif name == \"approx_greedy_coreset\":\n            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n\n    return (\"get_sampler\", get_sampler)\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_295-345", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n\n            patchcore_instance = patchcore.patchcore.PatchCore(device)\n            patchcore_instance.load(\n                backbone=backbone,\n                layers_to_extract_from=layers_to_extract_from,\n                device=device,\n                input_shape=input_shape,\n                pretrain_embed_dimension=pretrain_embed_dimension,\n                target_embed_dimension=target_embed_dimension,\n                patchsize=patchsize,\n                featuresampler=sampler,\n                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n                nn_method=nn_method,\n            )\n            loaded_patchcores.append(patchcore_instance)\n        return loaded_patchcores\n\n    return (\"get_patchcore\", get_patchcore)\n\n\n@main.command(\"sampler\")\n@click.argument(\"name\", type=str)\n@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\ndef sampler(name, percentage):\n    def get_sampler(device):\n        if name == \"identity\":\n            return patchcore.sampler.IdentitySampler()\n        elif name == \"greedy_coreset\":\n            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n        elif name == \"approx_greedy_coreset\":\n            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n\n    return (\"get_sampler\", get_sampler)\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name,\n    data_path,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_305-355", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                target_embed_dimension=target_embed_dimension,\n                patchsize=patchsize,\n                featuresampler=sampler,\n                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n                nn_method=nn_method,\n            )\n            loaded_patchcores.append(patchcore_instance)\n        return loaded_patchcores\n\n    return (\"get_patchcore\", get_patchcore)\n\n\n@main.command(\"sampler\")\n@click.argument(\"name\", type=str)\n@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\ndef sampler(name, percentage):\n    def get_sampler(device):\n        if name == \"identity\":\n            return patchcore.sampler.IdentitySampler()\n        elif name == \"greedy_coreset\":\n            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n        elif name == \"approx_greedy_coreset\":\n            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n\n    return (\"get_sampler\", get_sampler)\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name,\n    data_path,\n    subdatasets,\n    train_val_split,\n    batch_size,\n    resize,\n    imagesize,\n    num_workers,\n    augment,\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_315-365", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n\n@main.command(\"sampler\")\n@click.argument(\"name\", type=str)\n@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\ndef sampler(name, percentage):\n    def get_sampler(device):\n        if name == \"identity\":\n            return patchcore.sampler.IdentitySampler()\n        elif name == \"greedy_coreset\":\n            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n        elif name == \"approx_greedy_coreset\":\n            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n\n    return (\"get_sampler\", get_sampler)\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name,\n    data_path,\n    subdatasets,\n    train_val_split,\n    batch_size,\n    resize,\n    imagesize,\n    num_workers,\n    augment,\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders(seed):\n        dataloaders = []\n        for subdataset in subdatasets:\n            train_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                train_val_split=train_val_split,\n                imagesize=imagesize,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_325-375", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n        elif name == \"approx_greedy_coreset\":\n            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n\n    return (\"get_sampler\", get_sampler)\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name,\n    data_path,\n    subdatasets,\n    train_val_split,\n    batch_size,\n    resize,\n    imagesize,\n    num_workers,\n    augment,\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders(seed):\n        dataloaders = []\n        for subdataset in subdatasets:\n            train_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                train_val_split=train_val_split,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TRAIN,\n                seed=seed,\n                augment=augment,\n            )\n\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_335-385", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name,\n    data_path,\n    subdatasets,\n    train_val_split,\n    batch_size,\n    resize,\n    imagesize,\n    num_workers,\n    augment,\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders(seed):\n        dataloaders = []\n        for subdataset in subdatasets:\n            train_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                train_val_split=train_val_split,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TRAIN,\n                seed=seed,\n                augment=augment,\n            )\n\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            train_dataloader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_345-395", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "    subdatasets,\n    train_val_split,\n    batch_size,\n    resize,\n    imagesize,\n    num_workers,\n    augment,\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders(seed):\n        dataloaders = []\n        for subdataset in subdatasets:\n            train_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                train_val_split=train_val_split,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TRAIN,\n                seed=seed,\n                augment=augment,\n            )\n\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            train_dataloader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_355-405", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "\n    def get_dataloaders(seed):\n        dataloaders = []\n        for subdataset in subdatasets:\n            train_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                train_val_split=train_val_split,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TRAIN,\n                seed=seed,\n                augment=augment,\n            )\n\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            train_dataloader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            train_dataloader.name = name\n            if subdataset is not None:\n                train_dataloader.name += \"_\" + subdataset\n\n            if train_val_split < 1:\n                val_dataset = dataset_library.__dict__[dataset_info[1]](\n                    data_path,\n                    classname=subdataset,\n                    resize=resize,\n                    train_val_split=train_val_split,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_365-415", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                split=dataset_library.DatasetSplit.TRAIN,\n                seed=seed,\n                augment=augment,\n            )\n\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            train_dataloader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            train_dataloader.name = name\n            if subdataset is not None:\n                train_dataloader.name += \"_\" + subdataset\n\n            if train_val_split < 1:\n                val_dataset = dataset_library.__dict__[dataset_info[1]](\n                    data_path,\n                    classname=subdataset,\n                    resize=resize,\n                    train_val_split=train_val_split,\n                    imagesize=imagesize,\n                    split=dataset_library.DatasetSplit.VAL,\n                    seed=seed,\n                )\n\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_375-425", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            train_dataloader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            train_dataloader.name = name\n            if subdataset is not None:\n                train_dataloader.name += \"_\" + subdataset\n\n            if train_val_split < 1:\n                val_dataset = dataset_library.__dict__[dataset_info[1]](\n                    data_path,\n                    classname=subdataset,\n                    resize=resize,\n                    train_val_split=train_val_split,\n                    imagesize=imagesize,\n                    split=dataset_library.DatasetSplit.VAL,\n                    seed=seed,\n                )\n\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                    pin_memory=True,\n                )\n            else:\n                val_dataloader = None\n            dataloader_dict = {\n                \"training\": train_dataloader,\n                \"validation\": val_dataloader,\n                \"testing\": test_dataloader,\n            }\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_385-435", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            train_dataloader.name = name\n            if subdataset is not None:\n                train_dataloader.name += \"_\" + subdataset\n\n            if train_val_split < 1:\n                val_dataset = dataset_library.__dict__[dataset_info[1]](\n                    data_path,\n                    classname=subdataset,\n                    resize=resize,\n                    train_val_split=train_val_split,\n                    imagesize=imagesize,\n                    split=dataset_library.DatasetSplit.VAL,\n                    seed=seed,\n                )\n\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                    pin_memory=True,\n                )\n            else:\n                val_dataloader = None\n            dataloader_dict = {\n                \"training\": train_dataloader,\n                \"validation\": val_dataloader,\n                \"testing\": test_dataloader,\n            }\n\n            dataloaders.append(dataloader_dict)\n        return dataloaders\n\n    return (\"get_dataloaders\", get_dataloaders)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n    main()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_395-435", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "            train_dataloader.name = name\n            if subdataset is not None:\n                train_dataloader.name += \"_\" + subdataset\n\n            if train_val_split < 1:\n                val_dataset = dataset_library.__dict__[dataset_info[1]](\n                    data_path,\n                    classname=subdataset,\n                    resize=resize,\n                    train_val_split=train_val_split,\n                    imagesize=imagesize,\n                    split=dataset_library.DatasetSplit.VAL,\n                    seed=seed,\n                )\n\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                    pin_memory=True,\n                )\n            else:\n                val_dataloader = None\n            dataloader_dict = {\n                \"training\": train_dataloader,\n                \"validation\": val_dataloader,\n                \"testing\": test_dataloader,\n            }\n\n            dataloaders.append(dataloader_dict)\n        return dataloaders\n\n    return (\"get_dataloaders\", get_dataloaders)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n    main()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 435, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-run_patchcore.py_405-435", "title": "amazon-science_patchcore-inspection-bin-run_patchcore.py", "text": "                    imagesize=imagesize,\n                    split=dataset_library.DatasetSplit.VAL,\n                    seed=seed,\n                )\n\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                    pin_memory=True,\n                )\n            else:\n                val_dataloader = None\n            dataloader_dict = {\n                \"training\": train_dataloader,\n                \"validation\": val_dataloader,\n                \"testing\": test_dataloader,\n            }\n\n            dataloaders.append(dataloader_dict)\n        return dataloaders\n\n    return (\"get_dataloaders\", get_dataloaders)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n    main()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "run_patchcore.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 435, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_0-25", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "import contextlib\nimport gc\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_0-35", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "import contextlib\nimport gc\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--save_segmentation_images\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(methods, results_path, gpu, seed, save_segmentation_images):\n    methods = {key: item for (key, item) in methods}\n\n    os.makedirs(results_path, exist_ok=True)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_0-45", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "import contextlib\nimport gc\nimport logging\nimport os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--save_segmentation_images\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(methods, results_path, gpu, seed, save_segmentation_images):\n    methods = {key: item for (key, item) in methods}\n\n    os.makedirs(results_path, exist_ok=True)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_5-55", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\nimport click\nimport numpy as np\nimport torch\n\nimport patchcore.common\nimport patchcore.metrics\nimport patchcore.patchcore\nimport patchcore.sampler\nimport patchcore.utils\n\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--save_segmentation_images\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(methods, results_path, gpu, seed, save_segmentation_images):\n    methods = {key: item for (key, item) in methods}\n\n    os.makedirs(results_path, exist_ok=True)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n    dataloader_iter = dataloader_iter(seed)\n    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n    patchcore_iter = patchcore_iter(device)\n    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n        raise ValueError(\n            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_15-65", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\nLOGGER = logging.getLogger(__name__)\n\n_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n\n\n@click.group(chain=True)\n@click.argument(\"results_path\", type=str)\n@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n@click.option(\"--seed\", type=int, default=0, show_default=True)\n@click.option(\"--save_segmentation_images\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(methods, results_path, gpu, seed, save_segmentation_images):\n    methods = {key: item for (key, item) in methods}\n\n    os.makedirs(results_path, exist_ok=True)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n    dataloader_iter = dataloader_iter(seed)\n    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n    patchcore_iter = patchcore_iter(device)\n    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n        raise ValueError(\n            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"\n        )\n\n    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_25-75", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "@click.option(\"--save_segmentation_images\", is_flag=True)\ndef main(**kwargs):\n    pass\n\n\n@main.result_callback()\ndef run(methods, results_path, gpu, seed, save_segmentation_images):\n    methods = {key: item for (key, item) in methods}\n\n    os.makedirs(results_path, exist_ok=True)\n\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n    dataloader_iter = dataloader_iter(seed)\n    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n    patchcore_iter = patchcore_iter(device)\n    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n        raise ValueError(\n            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"\n        )\n\n    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"testing\"].name\n\n        with device_context:\n\n            torch.cuda.empty_cache()\n            if dataloader_count < n_patchcores:\n                PatchCore_list = next(patchcore_iter)\n\n            aggregator = {\"scores\": [], \"segmentations\": []}", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_35-85", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n    device = patchcore.utils.set_torch_device(gpu)\n    # Device context here is specifically set and used later\n    # because there was GPU memory-bleeding which I could only fix with\n    # context managers.\n    device_context = (\n        torch.cuda.device(\"cuda:{}\".format(device.index))\n        if \"cuda\" in device.type.lower()\n        else contextlib.suppress()\n    )\n\n    result_collect = []\n\n    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n    dataloader_iter = dataloader_iter(seed)\n    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n    patchcore_iter = patchcore_iter(device)\n    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n        raise ValueError(\n            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"\n        )\n\n    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"testing\"].name\n\n        with device_context:\n\n            torch.cuda.empty_cache()\n            if dataloader_count < n_patchcores:\n                PatchCore_list = next(patchcore_iter)\n\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_45-95", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n    result_collect = []\n\n    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n    dataloader_iter = dataloader_iter(seed)\n    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n    patchcore_iter = patchcore_iter(device)\n    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n        raise ValueError(\n            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"\n        )\n\n    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"testing\"].name\n\n        with device_context:\n\n            torch.cuda.empty_cache()\n            if dataloader_count < n_patchcores:\n                PatchCore_list = next(patchcore_iter)\n\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_55-105", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "        )\n\n    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n        LOGGER.info(\n            \"Evaluating dataset [{}] ({}/{})...\".format(\n                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n            )\n        )\n\n        patchcore.utils.fix_seeds(seed, device)\n\n        dataset_name = dataloaders[\"testing\"].name\n\n        with device_context:\n\n            torch.cuda.empty_cache()\n            if dataloader_count < n_patchcores:\n                PatchCore_list = next(patchcore_iter)\n\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_65-115", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n        dataset_name = dataloaders[\"testing\"].name\n\n        with device_context:\n\n            torch.cuda.empty_cache()\n            if dataloader_count < n_patchcores:\n                PatchCore_list = next(patchcore_iter)\n\n            aggregator = {\"scores\": [], \"segmentations\": []}\n            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # Plot Example Images.\n            if save_segmentation_images:\n                image_paths = [", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_75-125", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            for i, PatchCore in enumerate(PatchCore_list):\n                torch.cuda.empty_cache()\n                LOGGER.info(\n                    \"Embedding test data with models ({}/{})\".format(\n                        i + 1, len(PatchCore_list)\n                    )\n                )\n                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n                    dataloaders[\"testing\"]\n                )\n                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # Plot Example Images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_85-135", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                aggregator[\"scores\"].append(scores)\n                aggregator[\"segmentations\"].append(segmentations)\n\n            scores = np.array(aggregator[\"scores\"])\n            min_scores = scores.min(axis=-1).reshape(-1, 1)\n            max_scores = scores.max(axis=-1).reshape(-1, 1)\n            scores = (scores - min_scores) / (max_scores - min_scores)\n            scores = np.mean(scores, axis=0)\n\n            segmentations = np.array(aggregator[\"segmentations\"])\n            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # Plot Example Images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_95-145", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            min_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .min(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            max_scores = (\n                segmentations.reshape(len(segmentations), -1)\n                .max(axis=-1)\n                .reshape(-1, 1, 1, 1)\n            )\n            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # Plot Example Images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                patchcore.utils.plot_segmentation_images(\n                    results_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_105-155", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n            segmentations = np.mean(segmentations, axis=0)\n\n            anomaly_labels = [\n                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n            ]\n\n            # Plot Example Images.\n            if save_segmentation_images:\n                image_paths = [\n                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                patchcore.utils.plot_segmentation_images(\n                    results_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            # Compute Image-level AUROC scores for all images.\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_115-165", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n                mask_paths = [\n                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n                ]\n\n                def image_transform(image):\n                    in_std = np.array(\n                        dataloaders[\"testing\"].dataset.transform_std\n                    ).reshape(-1, 1, 1)\n                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                patchcore.utils.plot_segmentation_images(\n                    results_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            # Compute Image-level AUROC scores for all images.\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only for images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_125-175", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                    in_mean = np.array(\n                        dataloaders[\"testing\"].dataset.transform_mean\n                    ).reshape(-1, 1, 1)\n                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n                    return np.clip(\n                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n                    ).astype(np.uint8)\n\n                def mask_transform(mask):\n                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n\n                patchcore.utils.plot_segmentation_images(\n                    results_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            # Compute Image-level AUROC scores for all images.\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only for images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_135-185", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n                patchcore.utils.plot_segmentation_images(\n                    results_path,\n                    image_paths,\n                    segmentations,\n                    scores,\n                    mask_paths,\n                    image_transform=image_transform,\n                    mask_transform=mask_transform,\n                )\n\n            LOGGER.info(\"Computing evaluation metrics.\")\n            # Compute Image-level AUROC scores for all images.\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only for images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            del PatchCore_list\n            gc.collect()\n\n        LOGGER.info(\"\\n\\n-----\\n\")", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_145-195", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n            LOGGER.info(\"Computing evaluation metrics.\")\n            # Compute Image-level AUROC scores for all images.\n            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n                scores, anomaly_labels\n            )[\"auroc\"]\n\n            # Compute PRO score & PW Auroc for all images\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                segmentations, masks_gt\n            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only for images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            del PatchCore_list\n            gc.collect()\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        results_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_155-205", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            )\n            full_pixel_auroc = pixel_scores[\"auroc\"]\n\n            # Compute PRO score & PW Auroc only for images with anomalies\n            sel_idxs = []\n            for i in range(len(masks_gt)):\n                if np.sum(masks_gt[i]) > 0:\n                    sel_idxs.append(i)\n            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]\n            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            del PatchCore_list\n            gc.collect()\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        results_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core_loader\")\n# Pretraining-specific parameters.\n@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n    def get_patchcore_iter(device):", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_165-215", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            )\n            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n\n            result_collect.append(\n                {\n                    \"dataset_name\": dataset_name,\n                    \"instance_auroc\": auroc,\n                    \"full_pixel_auroc\": full_pixel_auroc,\n                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n                }\n            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            del PatchCore_list\n            gc.collect()\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        results_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core_loader\")\n# Pretraining-specific parameters.\n@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n    def get_patchcore_iter(device):\n        for patch_core_path in patch_core_paths:\n            loaded_patchcores = []\n            gc.collect()\n            n_patchcores = len(\n                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n            )\n            if n_patchcores == 1:\n                nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n                patchcore_instance = patchcore.patchcore.PatchCore(device)\n                patchcore_instance.load_from_path(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_175-225", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "            )\n\n            for key, item in result_collect[-1].items():\n                if key != \"dataset_name\":\n                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n\n            del PatchCore_list\n            gc.collect()\n\n        LOGGER.info(\"\\n\\n-----\\n\")\n\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        results_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core_loader\")\n# Pretraining-specific parameters.\n@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n    def get_patchcore_iter(device):\n        for patch_core_path in patch_core_paths:\n            loaded_patchcores = []\n            gc.collect()\n            n_patchcores = len(\n                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n            )\n            if n_patchcores == 1:\n                nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n                patchcore_instance = patchcore.patchcore.PatchCore(device)\n                patchcore_instance.load_from_path(\n                    load_path=patch_core_path, device=device, nn_method=nn_method\n                )\n                loaded_patchcores.append(patchcore_instance)\n            else:\n                for i in range(n_patchcores):\n                    nn_method = patchcore.common.FaissNN(\n                        faiss_on_gpu, faiss_num_workers\n                    )\n                    patchcore_instance = patchcore.patchcore.PatchCore(device)\n                    patchcore_instance.load_from_path(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_185-235", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n    result_metric_names = list(result_collect[-1].keys())[1:]\n    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n    result_scores = [list(results.values())[1:] for results in result_collect]\n    patchcore.utils.compute_and_store_final_results(\n        results_path,\n        result_scores,\n        column_names=result_metric_names,\n        row_names=result_dataset_names,\n    )\n\n\n@main.command(\"patch_core_loader\")\n# Pretraining-specific parameters.\n@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n    def get_patchcore_iter(device):\n        for patch_core_path in patch_core_paths:\n            loaded_patchcores = []\n            gc.collect()\n            n_patchcores = len(\n                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n            )\n            if n_patchcores == 1:\n                nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n                patchcore_instance = patchcore.patchcore.PatchCore(device)\n                patchcore_instance.load_from_path(\n                    load_path=patch_core_path, device=device, nn_method=nn_method\n                )\n                loaded_patchcores.append(patchcore_instance)\n            else:\n                for i in range(n_patchcores):\n                    nn_method = patchcore.common.FaissNN(\n                        faiss_on_gpu, faiss_num_workers\n                    )\n                    patchcore_instance = patchcore.patchcore.PatchCore(device)\n                    patchcore_instance.load_from_path(\n                        load_path=patch_core_path,\n                        device=device,\n                        nn_method=nn_method,\n                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n                    )\n                    loaded_patchcores.append(patchcore_instance)\n\n            yield loaded_patchcores\n\n    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_195-245", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n\n@main.command(\"patch_core_loader\")\n# Pretraining-specific parameters.\n@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n# NN on GPU.\n@click.option(\"--faiss_on_gpu\", is_flag=True)\n@click.option(\"--faiss_num_workers\", type=int, default=8)\ndef patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n    def get_patchcore_iter(device):\n        for patch_core_path in patch_core_paths:\n            loaded_patchcores = []\n            gc.collect()\n            n_patchcores = len(\n                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n            )\n            if n_patchcores == 1:\n                nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n                patchcore_instance = patchcore.patchcore.PatchCore(device)\n                patchcore_instance.load_from_path(\n                    load_path=patch_core_path, device=device, nn_method=nn_method\n                )\n                loaded_patchcores.append(patchcore_instance)\n            else:\n                for i in range(n_patchcores):\n                    nn_method = patchcore.common.FaissNN(\n                        faiss_on_gpu, faiss_num_workers\n                    )\n                    patchcore_instance = patchcore.patchcore.PatchCore(device)\n                    patchcore_instance.load_from_path(\n                        load_path=patch_core_path,\n                        device=device,\n                        nn_method=nn_method,\n                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n                    )\n                    loaded_patchcores.append(patchcore_instance)\n\n            yield loaded_patchcores\n\n    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_205-255", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "        for patch_core_path in patch_core_paths:\n            loaded_patchcores = []\n            gc.collect()\n            n_patchcores = len(\n                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n            )\n            if n_patchcores == 1:\n                nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n                patchcore_instance = patchcore.patchcore.PatchCore(device)\n                patchcore_instance.load_from_path(\n                    load_path=patch_core_path, device=device, nn_method=nn_method\n                )\n                loaded_patchcores.append(patchcore_instance)\n            else:\n                for i in range(n_patchcores):\n                    nn_method = patchcore.common.FaissNN(\n                        faiss_on_gpu, faiss_num_workers\n                    )\n                    patchcore_instance = patchcore.patchcore.PatchCore(device)\n                    patchcore_instance.load_from_path(\n                        load_path=patch_core_path,\n                        device=device,\n                        nn_method=nn_method,\n                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n                    )\n                    loaded_patchcores.append(patchcore_instance)\n\n            yield loaded_patchcores\n\n    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders_iter(seed):\n        for subdataset in subdatasets:\n            test_dataset = dataset_library.__dict__[dataset_info[1]](", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_215-265", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                    load_path=patch_core_path, device=device, nn_method=nn_method\n                )\n                loaded_patchcores.append(patchcore_instance)\n            else:\n                for i in range(n_patchcores):\n                    nn_method = patchcore.common.FaissNN(\n                        faiss_on_gpu, faiss_num_workers\n                    )\n                    patchcore_instance = patchcore.patchcore.PatchCore(device)\n                    patchcore_instance.load_from_path(\n                        load_path=patch_core_path,\n                        device=device,\n                        nn_method=nn_method,\n                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n                    )\n                    loaded_patchcores.append(patchcore_instance)\n\n            yield loaded_patchcores\n\n    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders_iter(seed):\n        for subdataset in subdatasets:\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_225-275", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                        load_path=patch_core_path,\n                        device=device,\n                        nn_method=nn_method,\n                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n                    )\n                    loaded_patchcores.append(patchcore_instance)\n\n            yield loaded_patchcores\n\n    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])\n\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders_iter(seed):\n        for subdataset in subdatasets:\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader.name = name\n            if subdataset is not None:\n                test_dataloader.name += \"_\" + subdataset\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_235-285", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "\n\n@main.command(\"dataset\")\n@click.argument(\"name\", type=str)\n@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n@click.option(\"--resize\", default=256, type=int, show_default=True)\n@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders_iter(seed):\n        for subdataset in subdatasets:\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader.name = name\n            if subdataset is not None:\n                test_dataloader.name += \"_\" + subdataset\n\n            dataloader_dict = {\"testing\": test_dataloader}\n\n            yield dataloader_dict\n\n    return (\"get_dataloaders_iter\", [get_dataloaders_iter, len(subdatasets)])\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_245-286", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "@click.option(\"--augment\", is_flag=True)\ndef dataset(\n    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n):\n    dataset_info = _DATASETS[name]\n    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n\n    def get_dataloaders_iter(seed):\n        for subdataset in subdatasets:\n            test_dataset = dataset_library.__dict__[dataset_info[1]](\n                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader.name = name\n            if subdataset is not None:\n                test_dataloader.name += \"_\" + subdataset\n\n            dataloader_dict = {\"testing\": test_dataloader}\n\n            yield dataloader_dict\n\n    return (\"get_dataloaders_iter\", [get_dataloaders_iter, len(subdatasets)])\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n    main()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 286, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py_255-286", "title": "amazon-science_patchcore-inspection-bin-load_and_evaluate_patchcore.py", "text": "                data_path,\n                classname=subdataset,\n                resize=resize,\n                imagesize=imagesize,\n                split=dataset_library.DatasetSplit.TEST,\n                seed=seed,\n            )\n\n            test_dataloader = torch.utils.data.DataLoader(\n                test_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                pin_memory=True,\n            )\n\n            test_dataloader.name = name\n            if subdataset is not None:\n                test_dataloader.name += \"_\" + subdataset\n\n            dataloader_dict = {\"testing\": test_dataloader}\n\n            yield dataloader_dict\n\n    return (\"get_dataloaders_iter\", [get_dataloaders_iter, len(subdatasets)])\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n    main()", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "bin", "load_and_evaluate_patchcore.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 286, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_0-25", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "import pytest\nimport torch\nimport torch.utils.data\n\nfrom patchcore import sampler\n\n\ndef _dummy_features(feature_dimension):\n    num_samples = 5000\n    return (\n        torch.arange(num_samples).unsqueeze(1)\n        / float(num_samples)\n        * torch.ones((num_samples, feature_dimension))\n    )\n\n\ndef _dummy_constant_features(number_of_examples, feature_dimension):\n    return torch.ones([number_of_examples, feature_dimension])\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_standard_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_0-35", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "import pytest\nimport torch\nimport torch.utils.data\n\nfrom patchcore import sampler\n\n\ndef _dummy_features(feature_dimension):\n    num_samples = 5000\n    return (\n        torch.arange(num_samples).unsqueeze(1)\n        / float(num_samples)\n        * torch.ones((num_samples, feature_dimension))\n    )\n\n\ndef _dummy_constant_features(number_of_examples, feature_dimension):\n    return torch.ones([number_of_examples, feature_dimension])\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_standard_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.GreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_0-45", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "import pytest\nimport torch\nimport torch.utils.data\n\nfrom patchcore import sampler\n\n\ndef _dummy_features(feature_dimension):\n    num_samples = 5000\n    return (\n        torch.arange(num_samples).unsqueeze(1)\n        / float(num_samples)\n        * torch.ones((num_samples, feature_dimension))\n    )\n\n\ndef _dummy_constant_features(number_of_examples, feature_dimension):\n    return torch.ones([number_of_examples, feature_dimension])\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_standard_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.GreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_approximate_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_5-55", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "\n\ndef _dummy_features(feature_dimension):\n    num_samples = 5000\n    return (\n        torch.arange(num_samples).unsqueeze(1)\n        / float(num_samples)\n        * torch.ones((num_samples, feature_dimension))\n    )\n\n\ndef _dummy_constant_features(number_of_examples, feature_dimension):\n    return torch.ones([number_of_examples, feature_dimension])\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_standard_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.GreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_approximate_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_15-65", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "\ndef _dummy_constant_features(number_of_examples, feature_dimension):\n    return torch.ones([number_of_examples, feature_dimension])\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_standard_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.GreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_approximate_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_coreset_sampling_on_same_samples():\n    feature_dimension = 2\n    init_features = _dummy_constant_features(5000, feature_dimension)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_25-75", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    sampling_percentage = 0.1\n    model = sampler.GreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_approximate_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_coreset_sampling_on_same_samples():\n    feature_dimension = 2\n    init_features = _dummy_constant_features(5000, feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_35-85", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_approximate_greedy_coreset_sampling():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_coreset_sampling_on_same_samples():\n    feature_dimension = 2\n    init_features = _dummy_constant_features(5000, feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert len(torch.unique(subsampled_features, dim=0)) == 1\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_random_sampling():\n    init_features = _dummy_features(feature_dimension=2)\n\n    sampling_percentage = 0.1", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_45-95", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_coreset_sampling_on_same_samples():\n    feature_dimension = 2\n    init_features = _dummy_constant_features(5000, feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert len(torch.unique(subsampled_features, dim=0)) == 1\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_random_sampling():\n    init_features = _dummy_features(feature_dimension=2)\n\n    sampling_percentage = 0.1\n    model = sampler.RandomSampler(percentage=sampling_percentage)\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_55-105", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_coreset_sampling_on_same_samples():\n    feature_dimension = 2\n    init_features = _dummy_constant_features(5000, feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert len(torch.unique(subsampled_features, dim=0)) == 1\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_random_sampling():\n    init_features = _dummy_features(feature_dimension=2)\n\n    sampling_percentage = 0.1\n    model = sampler.RandomSampler(percentage=sampling_percentage)\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_type_retention():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_65-115", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert len(torch.unique(subsampled_features, dim=0)) == 1\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_random_sampling():\n    init_features = _dummy_features(feature_dimension=2)\n\n    sampling_percentage = 0.1\n    model = sampler.RandomSampler(percentage=sampling_percentage)\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_type_retention():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert type(subsampled_features) == type(init_features)\n    assert subsampled_features.device == init_features.device\n\n    subsampled_features_numpy = model.run(init_features.numpy())\n    assert isinstance(subsampled_features_numpy, type(init_features.numpy()))\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_75-125", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert len(torch.unique(subsampled_features, dim=0)) == 1\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_random_sampling():\n    init_features = _dummy_features(feature_dimension=2)\n\n    sampling_percentage = 0.1\n    model = sampler.RandomSampler(percentage=sampling_percentage)\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_type_retention():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert type(subsampled_features) == type(init_features)\n    assert subsampled_features.device == init_features.device\n\n    subsampled_features_numpy = model.run(init_features.numpy())\n    assert isinstance(subsampled_features_numpy, type(init_features.numpy()))\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_johnsonlindenstrauss_reduction():\n    feature_dimension = 256\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    dimension_to_project_features_to = 64\n\n    model = sampler.ApproximateGreedyCoresetSampler(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_85-133", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "    model = sampler.RandomSampler(percentage=sampling_percentage)\n    subsampled_features = model.run(init_features)\n\n    target_num_subsampled_features = int(len(init_features) * sampling_percentage)\n    assert len(subsampled_features) == target_num_subsampled_features\n    assert (\n        len(torch.unique(subsampled_features, dim=0)) == target_num_subsampled_features\n    )\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_type_retention():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert type(subsampled_features) == type(init_features)\n    assert subsampled_features.device == init_features.device\n\n    subsampled_features_numpy = model.run(init_features.numpy())\n    assert isinstance(subsampled_features_numpy, type(init_features.numpy()))\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_johnsonlindenstrauss_reduction():\n    feature_dimension = 256\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    dimension_to_project_features_to = 64\n\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=dimension_to_project_features_to,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert subsampled_features.shape[-1] == feature_dimension", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 133, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_95-133", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_type_retention():\n    feature_dimension = 2\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert type(subsampled_features) == type(init_features)\n    assert subsampled_features.device == init_features.device\n\n    subsampled_features_numpy = model.run(init_features.numpy())\n    assert isinstance(subsampled_features_numpy, type(init_features.numpy()))\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_johnsonlindenstrauss_reduction():\n    feature_dimension = 256\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    dimension_to_project_features_to = 64\n\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=dimension_to_project_features_to,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert subsampled_features.shape[-1] == feature_dimension", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 133, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_sampler.py_105-133", "title": "amazon-science_patchcore-inspection-test-test_sampler.py", "text": "        dimension_to_project_features_to=feature_dimension,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert type(subsampled_features) == type(init_features)\n    assert subsampled_features.device == init_features.device\n\n    subsampled_features_numpy = model.run(init_features.numpy())\n    assert isinstance(subsampled_features_numpy, type(init_features.numpy()))\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Fails for non-GPU machine.\")\ndef test_johnsonlindenstrauss_reduction():\n    feature_dimension = 256\n    init_features = _dummy_features(feature_dimension)\n\n    sampling_percentage = 0.1\n    dimension_to_project_features_to = 64\n\n    model = sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=dimension_to_project_features_to,\n    )\n    subsampled_features = model.run(init_features)\n\n    assert subsampled_features.shape[-1] == feature_dimension", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_sampler.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 133, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_0-25", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "import numpy as np\nimport torch.utils.data\nfrom torchvision import models\n\nimport patchcore\nfrom patchcore import patchcore as patchcore_model\n\n\ndef _dummy_features(number_of_examples, shape_of_examples):\n    return torch.Tensor(\n        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n    )\n\n\ndef _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_various_features(number_of_examples, shape_of_examples):\n    images = torch.ones((number_of_examples, *shape_of_examples))\n    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n    for _ in range(images.ndim - 1):\n        multiplier = multiplier.unsqueeze(-1)\n    return multiplier * images", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_0-35", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "import numpy as np\nimport torch.utils.data\nfrom torchvision import models\n\nimport patchcore\nfrom patchcore import patchcore as patchcore_model\n\n\ndef _dummy_features(number_of_examples, shape_of_examples):\n    return torch.Tensor(\n        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n    )\n\n\ndef _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_various_features(number_of_examples, shape_of_examples):\n    images = torch.ones((number_of_examples, *shape_of_examples))\n    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n    for _ in range(images.ndim - 1):\n        multiplier = multiplier.unsqueeze(-1)\n    return multiplier * images\n\n\ndef _dummy_various_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_various_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_images(number_of_examples, image_shape):\n    torch.manual_seed(0)\n    return torch.rand([number_of_examples, *image_shape])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_0-45", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "import numpy as np\nimport torch.utils.data\nfrom torchvision import models\n\nimport patchcore\nfrom patchcore import patchcore as patchcore_model\n\n\ndef _dummy_features(number_of_examples, shape_of_examples):\n    return torch.Tensor(\n        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n    )\n\n\ndef _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_various_features(number_of_examples, shape_of_examples):\n    images = torch.ones((number_of_examples, *shape_of_examples))\n    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n    for _ in range(images.ndim - 1):\n        multiplier = multiplier.unsqueeze(-1)\n    return multiplier * images\n\n\ndef _dummy_various_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_various_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_images(number_of_examples, image_shape):\n    torch.manual_seed(0)\n    return torch.rand([number_of_examples, *image_shape])\n\n\ndef _dummy_image_random_dataloader(number_of_examples, image_shape):\n    images = _dummy_images(number_of_examples, image_shape)\n    return torch.utils.data.DataLoader(images, batch_size=4)\n\n\ndef _standard_patchcore(image_dimension):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    backbone = models.wide_resnet50_2(pretrained=False)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_5-55", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "from patchcore import patchcore as patchcore_model\n\n\ndef _dummy_features(number_of_examples, shape_of_examples):\n    return torch.Tensor(\n        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n    )\n\n\ndef _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_various_features(number_of_examples, shape_of_examples):\n    images = torch.ones((number_of_examples, *shape_of_examples))\n    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n    for _ in range(images.ndim - 1):\n        multiplier = multiplier.unsqueeze(-1)\n    return multiplier * images\n\n\ndef _dummy_various_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_various_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_images(number_of_examples, image_shape):\n    torch.manual_seed(0)\n    return torch.rand([number_of_examples, *image_shape])\n\n\ndef _dummy_image_random_dataloader(number_of_examples, image_shape):\n    images = _dummy_images(number_of_examples, image_shape)\n    return torch.utils.data.DataLoader(images, batch_size=4)\n\n\ndef _standard_patchcore(image_dimension):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    backbone = models.wide_resnet50_2(pretrained=False)\n    backbone.name, backbone.seed = \"wideresnet50\", 0\n    patchcore_instance.load(\n        backbone=backbone,\n        layers_to_extract_from=[\"layer2\", \"layer3\"],\n        device=torch.device(\"cpu\"),\n        input_shape=[3, image_dimension, image_dimension],\n        pretrain_embed_dimension=1024,\n        target_embed_dimension=1024,\n        patchsize=3,\n        patchstride=1,", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_15-65", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "    features = _dummy_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_various_features(number_of_examples, shape_of_examples):\n    images = torch.ones((number_of_examples, *shape_of_examples))\n    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n    for _ in range(images.ndim - 1):\n        multiplier = multiplier.unsqueeze(-1)\n    return multiplier * images\n\n\ndef _dummy_various_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_various_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_images(number_of_examples, image_shape):\n    torch.manual_seed(0)\n    return torch.rand([number_of_examples, *image_shape])\n\n\ndef _dummy_image_random_dataloader(number_of_examples, image_shape):\n    images = _dummy_images(number_of_examples, image_shape)\n    return torch.utils.data.DataLoader(images, batch_size=4)\n\n\ndef _standard_patchcore(image_dimension):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    backbone = models.wide_resnet50_2(pretrained=False)\n    backbone.name, backbone.seed = \"wideresnet50\", 0\n    patchcore_instance.load(\n        backbone=backbone,\n        layers_to_extract_from=[\"layer2\", \"layer3\"],\n        device=torch.device(\"cpu\"),\n        input_shape=[3, image_dimension, image_dimension],\n        pretrain_embed_dimension=1024,\n        target_embed_dimension=1024,\n        patchsize=3,\n        patchstride=1,\n        spade_nn=2,\n    )\n    return patchcore_instance\n\n\ndef _load_patchcore_from_path(load_path):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    patchcore_instance.load_from_path(\n        load_path=load_path,\n        device=torch.device(\"cpu\"),", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_25-75", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "\n\ndef _dummy_various_dataloader(number_of_examples, shape_of_examples):\n    features = _dummy_various_features(number_of_examples, shape_of_examples)\n    return torch.utils.data.DataLoader(features, batch_size=1)\n\n\ndef _dummy_images(number_of_examples, image_shape):\n    torch.manual_seed(0)\n    return torch.rand([number_of_examples, *image_shape])\n\n\ndef _dummy_image_random_dataloader(number_of_examples, image_shape):\n    images = _dummy_images(number_of_examples, image_shape)\n    return torch.utils.data.DataLoader(images, batch_size=4)\n\n\ndef _standard_patchcore(image_dimension):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    backbone = models.wide_resnet50_2(pretrained=False)\n    backbone.name, backbone.seed = \"wideresnet50\", 0\n    patchcore_instance.load(\n        backbone=backbone,\n        layers_to_extract_from=[\"layer2\", \"layer3\"],\n        device=torch.device(\"cpu\"),\n        input_shape=[3, image_dimension, image_dimension],\n        pretrain_embed_dimension=1024,\n        target_embed_dimension=1024,\n        patchsize=3,\n        patchstride=1,\n        spade_nn=2,\n    )\n    return patchcore_instance\n\n\ndef _load_patchcore_from_path(load_path):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    patchcore_instance.load_from_path(\n        load_path=load_path,\n        device=torch.device(\"cpu\"),\n        prepend=\"temp_patchcore\",\n        nn_method=patchcore.common.FaissNN(False, 4),\n    )\n    return patchcore_instance\n\n\ndef _approximate_greedycoreset_sampler_with_reduction(\n    sampling_percentage, johnsonlindenstrauss_dim\n):\n    return patchcore.sampler.ApproximateGreedyCoresetSampler(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_35-85", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "\n\ndef _dummy_image_random_dataloader(number_of_examples, image_shape):\n    images = _dummy_images(number_of_examples, image_shape)\n    return torch.utils.data.DataLoader(images, batch_size=4)\n\n\ndef _standard_patchcore(image_dimension):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    backbone = models.wide_resnet50_2(pretrained=False)\n    backbone.name, backbone.seed = \"wideresnet50\", 0\n    patchcore_instance.load(\n        backbone=backbone,\n        layers_to_extract_from=[\"layer2\", \"layer3\"],\n        device=torch.device(\"cpu\"),\n        input_shape=[3, image_dimension, image_dimension],\n        pretrain_embed_dimension=1024,\n        target_embed_dimension=1024,\n        patchsize=3,\n        patchstride=1,\n        spade_nn=2,\n    )\n    return patchcore_instance\n\n\ndef _load_patchcore_from_path(load_path):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    patchcore_instance.load_from_path(\n        load_path=load_path,\n        device=torch.device(\"cpu\"),\n        prepend=\"temp_patchcore\",\n        nn_method=patchcore.common.FaissNN(False, 4),\n    )\n    return patchcore_instance\n\n\ndef _approximate_greedycoreset_sampler_with_reduction(\n    sampling_percentage, johnsonlindenstrauss_dim\n):\n    return patchcore.sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n    )\n\n\ndef test_dummy_patchcore():\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_45-95", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "    backbone.name, backbone.seed = \"wideresnet50\", 0\n    patchcore_instance.load(\n        backbone=backbone,\n        layers_to_extract_from=[\"layer2\", \"layer3\"],\n        device=torch.device(\"cpu\"),\n        input_shape=[3, image_dimension, image_dimension],\n        pretrain_embed_dimension=1024,\n        target_embed_dimension=1024,\n        patchsize=3,\n        patchstride=1,\n        spade_nn=2,\n    )\n    return patchcore_instance\n\n\ndef _load_patchcore_from_path(load_path):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    patchcore_instance.load_from_path(\n        load_path=load_path,\n        device=torch.device(\"cpu\"),\n        prepend=\"temp_patchcore\",\n        nn_method=patchcore.common.FaissNN(False, 4),\n    )\n    return patchcore_instance\n\n\ndef _approximate_greedycoreset_sampler_with_reduction(\n    sampling_percentage, johnsonlindenstrauss_dim\n):\n    return patchcore.sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n    )\n\n\ndef test_dummy_patchcore():\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    print(model.featuresampler)\n    model.fit(training_dataloader)\n\n    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n    scores, masks = model.predict(test_features)\n\n    assert all([score > 0 for score in scores])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_55-105", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "        spade_nn=2,\n    )\n    return patchcore_instance\n\n\ndef _load_patchcore_from_path(load_path):\n    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n    patchcore_instance.load_from_path(\n        load_path=load_path,\n        device=torch.device(\"cpu\"),\n        prepend=\"temp_patchcore\",\n        nn_method=patchcore.common.FaissNN(False, 4),\n    )\n    return patchcore_instance\n\n\ndef _approximate_greedycoreset_sampler_with_reduction(\n    sampling_percentage, johnsonlindenstrauss_dim\n):\n    return patchcore.sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n    )\n\n\ndef test_dummy_patchcore():\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    print(model.featuresampler)\n    model.fit(training_dataloader)\n\n    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n    scores, masks = model.predict(test_features)\n\n    assert all([score > 0 for score in scores])\n    for mask in masks:\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_on_dataloader():\n    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_65-115", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "        prepend=\"temp_patchcore\",\n        nn_method=patchcore.common.FaissNN(False, 4),\n    )\n    return patchcore_instance\n\n\ndef _approximate_greedycoreset_sampler_with_reduction(\n    sampling_percentage, johnsonlindenstrauss_dim\n):\n    return patchcore.sampler.ApproximateGreedyCoresetSampler(\n        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n    )\n\n\ndef test_dummy_patchcore():\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    print(model.featuresampler)\n    model.fit(training_dataloader)\n\n    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n    scores, masks = model.predict(test_features)\n\n    assert all([score > 0 for score in scores])\n    for mask in masks:\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_on_dataloader():\n    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n\n    assert all([score < 1e-3 for score in scores])\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_75-125", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "        percentage=sampling_percentage,\n        device=torch.device(\"cpu\"),\n        number_of_starting_points=10,\n        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n    )\n\n\ndef test_dummy_patchcore():\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    print(model.featuresampler)\n    model.fit(training_dataloader)\n\n    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n    scores, masks = model.predict(test_features)\n\n    assert all([score > 0 for score in scores])\n    for mask in masks:\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_on_dataloader():\n    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n\n    assert all([score < 1e-3 for score in scores])\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_load_and_saveing(tmpdir):\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    model.save_to_path(tmpdir, \"temp_patchcore\")", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_85-135", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    print(model.featuresampler)\n    model.fit(training_dataloader)\n\n    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n    scores, masks = model.predict(test_features)\n\n    assert all([score > 0 for score in scores])\n    for mask in masks:\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_on_dataloader():\n    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n\n    assert all([score < 1e-3 for score in scores])\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_load_and_saveing(tmpdir):\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    model.save_to_path(tmpdir, \"temp_patchcore\")\n\n    test_features = torch.Tensor(\n        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n    )\n    scores, masks = model.predict(test_features)\n    other_scores, other_masks = model.predict(test_features)\n\n    assert np.all(scores == other_scores)\n    for mask, other_mask in zip(masks, other_masks):\n        assert np.all(mask == other_mask)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_95-145", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "    for mask in masks:\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_on_dataloader():\n    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n\n    assert all([score < 1e-3 for score in scores])\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_load_and_saveing(tmpdir):\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    model.save_to_path(tmpdir, \"temp_patchcore\")\n\n    test_features = torch.Tensor(\n        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n    )\n    scores, masks = model.predict(test_features)\n    other_scores, other_masks = model.predict(test_features)\n\n    assert np.all(scores == other_scores)\n    for mask, other_mask in zip(masks, other_masks):\n        assert np.all(mask == other_mask)\n\n\ndef test_patchcore_real_data():\n    image_dimension = 112\n    sampling_percentage = 0.1\n    model = _standard_patchcore(image_dimension)\n    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n        sampling_percentage=sampling_percentage,\n        johnsonlindenstrauss_dim=64,\n    )", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_105-155", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n\n    assert all([score < 1e-3 for score in scores])\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n\ndef test_patchcore_load_and_saveing(tmpdir):\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    model.save_to_path(tmpdir, \"temp_patchcore\")\n\n    test_features = torch.Tensor(\n        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n    )\n    scores, masks = model.predict(test_features)\n    other_scores, other_masks = model.predict(test_features)\n\n    assert np.all(scores == other_scores)\n    for mask, other_mask in zip(masks, other_masks):\n        assert np.all(mask == other_mask)\n\n\ndef test_patchcore_real_data():\n    image_dimension = 112\n    sampling_percentage = 0.1\n    model = _standard_patchcore(image_dimension)\n    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n        sampling_percentage=sampling_percentage,\n        johnsonlindenstrauss_dim=64,\n    )\n\n    num_dummy_train_images = 50\n    training_dataloader = _dummy_various_dataloader(\n        num_dummy_train_images, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n\n    num_dummy_test_images = 5\n    test_dataloader = _dummy_various_dataloader(\n        num_dummy_test_images, [3, image_dimension, image_dimension]", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_115-163", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "\ndef test_patchcore_load_and_saveing(tmpdir):\n    image_dimension = 112\n    model = _standard_patchcore(image_dimension)\n\n    training_dataloader = _dummy_constant_dataloader(\n        4, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n    model.save_to_path(tmpdir, \"temp_patchcore\")\n\n    test_features = torch.Tensor(\n        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n    )\n    scores, masks = model.predict(test_features)\n    other_scores, other_masks = model.predict(test_features)\n\n    assert np.all(scores == other_scores)\n    for mask, other_mask in zip(masks, other_masks):\n        assert np.all(mask == other_mask)\n\n\ndef test_patchcore_real_data():\n    image_dimension = 112\n    sampling_percentage = 0.1\n    model = _standard_patchcore(image_dimension)\n    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n        sampling_percentage=sampling_percentage,\n        johnsonlindenstrauss_dim=64,\n    )\n\n    num_dummy_train_images = 50\n    training_dataloader = _dummy_various_dataloader(\n        num_dummy_train_images, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n\n    num_dummy_test_images = 5\n    test_dataloader = _dummy_various_dataloader(\n        num_dummy_test_images, [3, image_dimension, image_dimension]\n    )\n    scores, masks, labels_gt, masks_gt = model.predict(test_dataloader)\n\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n    assert len(scores) == 5", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 163, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_125-163", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "\n    test_features = torch.Tensor(\n        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n    )\n    scores, masks = model.predict(test_features)\n    other_scores, other_masks = model.predict(test_features)\n\n    assert np.all(scores == other_scores)\n    for mask, other_mask in zip(masks, other_masks):\n        assert np.all(mask == other_mask)\n\n\ndef test_patchcore_real_data():\n    image_dimension = 112\n    sampling_percentage = 0.1\n    model = _standard_patchcore(image_dimension)\n    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n        sampling_percentage=sampling_percentage,\n        johnsonlindenstrauss_dim=64,\n    )\n\n    num_dummy_train_images = 50\n    training_dataloader = _dummy_various_dataloader(\n        num_dummy_train_images, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n\n    num_dummy_test_images = 5\n    test_dataloader = _dummy_various_dataloader(\n        num_dummy_test_images, [3, image_dimension, image_dimension]\n    )\n    scores, masks, labels_gt, masks_gt = model.predict(test_dataloader)\n\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n    assert len(scores) == 5", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 163, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_patchcore.py_135-163", "title": "amazon-science_patchcore-inspection-test-test_patchcore.py", "text": "\n\ndef test_patchcore_real_data():\n    image_dimension = 112\n    sampling_percentage = 0.1\n    model = _standard_patchcore(image_dimension)\n    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n        sampling_percentage=sampling_percentage,\n        johnsonlindenstrauss_dim=64,\n    )\n\n    num_dummy_train_images = 50\n    training_dataloader = _dummy_various_dataloader(\n        num_dummy_train_images, [3, image_dimension, image_dimension]\n    )\n    model.fit(training_dataloader)\n\n    num_dummy_test_images = 5\n    test_dataloader = _dummy_various_dataloader(\n        num_dummy_test_images, [3, image_dimension, image_dimension]\n    )\n    scores, masks, labels_gt, masks_gt = model.predict(test_dataloader)\n\n    for mask, mask_gt in zip(masks, masks_gt):\n        assert np.all(mask.shape == (image_dimension, image_dimension))\n        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n\n    assert len(scores) == 5", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_patchcore.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 163, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_0-25", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "import numpy as np\nimport pytest\n\nfrom patchcore import common\n\n\ndef test_calling_without_setting_index():\n    query = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    index = 2 * query\n\n    nn_search = common.FaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_approximate_faiss():\n    query = np.ones([768, 128], dtype=np.float32)\n    index = 2 * query", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_0-35", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "import numpy as np\nimport pytest\n\nfrom patchcore import common\n\n\ndef test_calling_without_setting_index():\n    query = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    index = 2 * query\n\n    nn_search = common.FaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_approximate_faiss():\n    query = np.ones([768, 128], dtype=np.float32)\n    index = 2 * query\n\n    nn_search = common.ApproximateFaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_0-45", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "import numpy as np\nimport pytest\n\nfrom patchcore import common\n\n\ndef test_calling_without_setting_index():\n    query = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    index = 2 * query\n\n    nn_search = common.FaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_approximate_faiss():\n    query = np.ones([768, 128], dtype=np.float32)\n    index = 2 * query\n\n    nn_search = common.ApproximateFaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_search_without_index_raises_exception():\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_search = common.FaissNN(on_gpu=False, num_workers=4)\n    with pytest.raises(AttributeError):\n        nn_search.run(2, features)\n    assert nn_search.run(2, features, features) is not None\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_5-55", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "\ndef test_calling_without_setting_index():\n    query = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    index = 2 * query\n\n    nn_search = common.FaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_approximate_faiss():\n    query = np.ones([768, 128], dtype=np.float32)\n    index = 2 * query\n\n    nn_search = common.ApproximateFaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_search_without_index_raises_exception():\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_search = common.FaissNN(on_gpu=False, num_workers=4)\n    with pytest.raises(AttributeError):\n        nn_search.run(2, features)\n    assert nn_search.run(2, features, features) is not None\n\n\ndef test_read_write_index(tmpdir):\n    index_filename = (tmpdir / \"index\").strpath\n    nn_model = common.FaissNN()\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_model.fit(features)\n    nn_model.save(index_filename)\n\n    loaded_nn_model = common.FaissNN()\n    loaded_nn_model.load(index_filename)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_15-65", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_approximate_faiss():\n    query = np.ones([768, 128], dtype=np.float32)\n    index = 2 * query\n\n    nn_search = common.ApproximateFaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_search_without_index_raises_exception():\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_search = common.FaissNN(on_gpu=False, num_workers=4)\n    with pytest.raises(AttributeError):\n        nn_search.run(2, features)\n    assert nn_search.run(2, features, features) is not None\n\n\ndef test_read_write_index(tmpdir):\n    index_filename = (tmpdir / \"index\").strpath\n    nn_model = common.FaissNN()\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_model.fit(features)\n    nn_model.save(index_filename)\n\n    loaded_nn_model = common.FaissNN()\n    loaded_nn_model.load(index_filename)\n\n    query_features = np.arange(10 * 6, dtype=np.float32).reshape(10, 6)\n    assert loaded_nn_model.run(2, query_features) is not None\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[0] == nn_model.run(2, query_features)[0]\n    )\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[1] == nn_model.run(2, query_features)[1]\n    )\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_25-75", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "\n    nn_search = common.ApproximateFaissNN()\n\n    distances_before_set_index, nn_indices_before_set_index = nn_search.run(\n        2, query, index\n    )\n    nn_search.fit(index)\n    distances_after_set_index, nn_indices_after_set_index = nn_search.run(2, query)\n\n    assert np.all(distances_before_set_index == distances_after_set_index)\n    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_search_without_index_raises_exception():\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_search = common.FaissNN(on_gpu=False, num_workers=4)\n    with pytest.raises(AttributeError):\n        nn_search.run(2, features)\n    assert nn_search.run(2, features, features) is not None\n\n\ndef test_read_write_index(tmpdir):\n    index_filename = (tmpdir / \"index\").strpath\n    nn_model = common.FaissNN()\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_model.fit(features)\n    nn_model.save(index_filename)\n\n    loaded_nn_model = common.FaissNN()\n    loaded_nn_model.load(index_filename)\n\n    query_features = np.arange(10 * 6, dtype=np.float32).reshape(10, 6)\n    assert loaded_nn_model.run(2, query_features) is not None\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[0] == nn_model.run(2, query_features)[0]\n    )\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[1] == nn_model.run(2, query_features)[1]\n    )\n\n\ndef test_average_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3))\n", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_35-85", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "    assert np.all(nn_indices_before_set_index == nn_indices_after_set_index)\n\n\ndef test_search_without_index_raises_exception():\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_search = common.FaissNN(on_gpu=False, num_workers=4)\n    with pytest.raises(AttributeError):\n        nn_search.run(2, features)\n    assert nn_search.run(2, features, features) is not None\n\n\ndef test_read_write_index(tmpdir):\n    index_filename = (tmpdir / \"index\").strpath\n    nn_model = common.FaissNN()\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_model.fit(features)\n    nn_model.save(index_filename)\n\n    loaded_nn_model = common.FaissNN()\n    loaded_nn_model.load(index_filename)\n\n    query_features = np.arange(10 * 6, dtype=np.float32).reshape(10, 6)\n    assert loaded_nn_model.run(2, query_features) is not None\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[0] == nn_model.run(2, query_features)[0]\n    )\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[1] == nn_model.run(2, query_features)[1]\n    )\n\n\ndef test_average_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 7))\n\n\ndef test_average_merger_output():\n    input_features = [np.ones([2, 3, 4, 5])]\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_45-95", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "\ndef test_read_write_index(tmpdir):\n    index_filename = (tmpdir / \"index\").strpath\n    nn_model = common.FaissNN()\n    features = np.arange(3 * 6, dtype=np.float32).reshape(3, 6)\n    nn_model.fit(features)\n    nn_model.save(index_filename)\n\n    loaded_nn_model = common.FaissNN()\n    loaded_nn_model.load(index_filename)\n\n    query_features = np.arange(10 * 6, dtype=np.float32).reshape(10, 6)\n    assert loaded_nn_model.run(2, query_features) is not None\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[0] == nn_model.run(2, query_features)[0]\n    )\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[1] == nn_model.run(2, query_features)[1]\n    )\n\n\ndef test_average_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 7))\n\n\ndef test_average_merger_output():\n    input_features = [np.ones([2, 3, 4, 5])]\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features == 1.0)\n\n\ndef test_concat_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_55-105", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "\n    query_features = np.arange(10 * 6, dtype=np.float32).reshape(10, 6)\n    assert loaded_nn_model.run(2, query_features) is not None\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[0] == nn_model.run(2, query_features)[0]\n    )\n    assert np.all(\n        loaded_nn_model.run(2, query_features)[1] == nn_model.run(2, query_features)[1]\n    )\n\n\ndef test_average_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 7))\n\n\ndef test_average_merger_output():\n    input_features = [np.ones([2, 3, 4, 5])]\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features == 1.0)\n\n\ndef test_concat_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3 * 4 * 5))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 3 * 4 * 5 + 4 * 3 * 5))\n\n\ndef test_concat_merger_output():\n    input_features = []\n    input_features.append(np.ones([2, 3, 4, 5]))", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_65-115", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "\ndef test_average_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3))\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 7))\n\n\ndef test_average_merger_output():\n    input_features = [np.ones([2, 3, 4, 5])]\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features == 1.0)\n\n\ndef test_concat_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3 * 4 * 5))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 3 * 4 * 5 + 4 * 3 * 5))\n\n\ndef test_concat_merger_output():\n    input_features = []\n    input_features.append(np.ones([2, 3, 4, 5]))\n    input_features.append(2 * np.ones([2, 3, 4, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features == 1.0)\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features[:, : 3 * 4 * 5] == 1.0)\n    assert np.all(output_features[:, 3 * 4 * 5 :] == 2.0)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_75-115", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 7))\n\n\ndef test_average_merger_output():\n    input_features = [np.ones([2, 3, 4, 5])]\n\n    merger = common.AverageMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features == 1.0)\n\n\ndef test_concat_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3 * 4 * 5))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 3 * 4 * 5 + 4 * 3 * 5))\n\n\ndef test_concat_merger_output():\n    input_features = []\n    input_features.append(np.ones([2, 3, 4, 5]))\n    input_features.append(2 * np.ones([2, 3, 4, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features == 1.0)\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features[:, : 3 * 4 * 5] == 1.0)\n    assert np.all(output_features[:, 3 * 4 * 5 :] == 2.0)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
{"_id": "amazon-science_patchcore-inspection_amazon-science_patchcore-inspection-test-test_common.py_85-115", "title": "amazon-science_patchcore-inspection-test-test_common.py", "text": "    assert np.all(output_features == 1.0)\n\n\ndef test_concat_merger_shape():\n    input_features = []\n    input_features.append(np.arange(2 * 3 * 4 * 5).reshape([2, 3, 4, 5]))\n    input_features.append(2 * np.arange(2 * 3 * 4 * 5).reshape([2, 4, 3, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features.shape == (2, 3 * 4 * 5))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features.shape == (2, 3 * 4 * 5 + 4 * 3 * 5))\n\n\ndef test_concat_merger_output():\n    input_features = []\n    input_features.append(np.ones([2, 3, 4, 5]))\n    input_features.append(2 * np.ones([2, 3, 4, 5]))\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge([input_features[0]])\n    assert np.all(output_features == 1.0)\n\n    merger = common.ConcatMerger()\n    output_features = merger.merge(input_features)\n    assert np.all(output_features[:, : 3 * 4 * 5] == 1.0)\n    assert np.all(output_features[:, 3 * 4 * 5 :] == 2.0)", "metadata": [{"fpath_tuple": ["amazon-science_patchcore-inspection", "test", "test_common.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 115, "window_size": 50, "repo": "amazon-science_patchcore-inspection", "slice_size": 5}]}
