{"_id": "amazon-science_patchcore-inspection/0", "text": "import abc\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):", "metadata": {"task_id": "amazon-science_patchcore-inspection/0", "ground_truth": "        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "context_start_lineno": 0, "lineno": 17, "function_name": "__init__", "line_no": 17}}
{"_id": "amazon-science_patchcore-inspection/1", "text": "import abc\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\n\nclass IdentitySampler:\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        return features\n\n\nclass BaseSampler(abc.ABC):\n    def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n\n    @abc.abstractmethod\n    def run(\n        self, features: Union[torch.Tensor, np.ndarray]\n    ) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n        self.features_is_numpy = isinstance(features, np.ndarray)\n        if not self.features_is_numpy:\n            self.features_device = features.device\n\n    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n        if self.features_is_numpy:\n            return features.cpu().numpy()\n        return features.to(self.features_device)\n\n\nclass GreedyCoresetSampler(BaseSampler):\n    def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/1", "ground_truth": "        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "__init__", "line_no": 46}}
{"_id": "amazon-science_patchcore-inspection/2", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):", "metadata": {"task_id": "amazon-science_patchcore-inspection/2", "ground_truth": "            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "_detach", "line_no": 94}}
{"_id": "amazon-science_patchcore-inspection/3", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/3", "ground_truth": "        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "_fill_memory_bank", "line_no": 156}}
{"_id": "amazon-science_patchcore-inspection/4", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):", "metadata": {"task_id": "amazon-science_patchcore-inspection/4", "ground_truth": "            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 159, "function_name": "_image_to_features", "line_no": 159}}
{"_id": "amazon-science_patchcore-inspection/5", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):", "metadata": {"task_id": "amazon-science_patchcore-inspection/5", "ground_truth": "        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 178, "function_name": "predict", "line_no": 178}}
{"_id": "amazon-science_patchcore-inspection/6", "text": "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\nimport logging\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport tqdm\n\nimport patchcore\nimport patchcore.backbones\nimport patchcore.common\nimport patchcore.sampler\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass PatchCore(torch.nn.Module):\n    def __init__(self, device):\n        \"\"\"PatchCore anomaly detection class.\"\"\"\n        super(PatchCore, self).__init__()\n        self.device = device\n\n    def load(\n        self,\n        backbone,\n        layers_to_extract_from,\n        device,\n        input_shape,\n        pretrain_embed_dimension,\n        target_embed_dimension,\n        patchsize=3,\n        patchstride=1,\n        anomaly_score_num_nn=1,\n        featuresampler=patchcore.sampler.IdentitySampler(),\n        nn_method=patchcore.common.FaissNN(False, 4),\n        **kwargs,\n    ):\n        self.backbone = backbone.to(device)\n        self.layers_to_extract_from = layers_to_extract_from\n        self.input_shape = input_shape\n\n        self.device = device\n        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n\n        self.forward_modules = torch.nn.ModuleDict({})\n\n        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n            self.backbone, self.layers_to_extract_from, self.device\n        )\n        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n\n        preprocessing = patchcore.common.Preprocessing(\n            feature_dimensions, pretrain_embed_dimension\n        )\n        self.forward_modules[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/6", "ground_truth": "        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 184, "function_name": "_predict_dataloader", "line_no": 184}}
{"_id": "amazon-science_patchcore-inspection/7", "text": "[\"preprocessing\"] = preprocessing\n\n        self.target_embed_dimension = target_embed_dimension\n        preadapt_aggregator = patchcore.common.Aggregator(\n            target_dim=target_embed_dimension\n        )\n\n        _ = preadapt_aggregator.to(self.device)\n\n        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n\n        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n        )\n\n        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n            device=self.device, target_size=input_shape[-2:]\n        )\n\n        self.featuresampler = featuresampler\n\n    def embed(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            features = []\n            for image in data:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                with torch.no_grad():\n                    input_image = image.to(torch.float).to(self.device)\n                    features.append(self._embed(input_image))\n            return features\n        return self._embed(data)\n\n    def _embed(self, images, detach=True, provide_patch_shapes=False):\n        \"\"\"Returns feature embeddings for images.\"\"\"\n\n        def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n\n        _ = self.forward_modules[\"feature_aggregator\"].eval()\n        with torch.no_grad():\n            features = self.forward_modules[\"feature_aggregator\"](images)\n\n        features = [features[layer] for layer in self.layers_to_extract_from]\n\n        features = [\n            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n        ]\n        patch_shapes = [x[1] for x in features]\n        features = [x[0] for x in features]\n        ref_num_patches = patch_shapes[0]\n\n        for i in range(1, len(features)):\n            _features = features[i]\n            patch_dims = patch_shapes[i]\n\n            # TODO(pgehler): Add comments\n            _features = _features.reshape(\n                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n            )\n            _features = _features.permute(0, -3, -2, -1, 1, 2)\n            perm_base_shape = _features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:", "metadata": {"task_id": "amazon-science_patchcore-inspection/7", "ground_truth": "        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 57, "lineno": 234, "function_name": "save_to_path", "line_no": 234}}
{"_id": "amazon-science_patchcore-inspection/8", "text": "features.shape\n            _features = _features.reshape(-1, *_features.shape[-2:])\n            _features = F.interpolate(\n                _features.unsqueeze(1),\n                size=(ref_num_patches[0], ref_num_patches[1]),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            _features = _features.squeeze(1)\n            _features = _features.reshape(\n                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n            )\n            _features = _features.permute(0, -2, -1, 1, 2, 3)\n            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n            features[i] = _features\n        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n\n        # As different feature backbones & patching provide differently\n        # sized features, these are brought into the correct form here.\n        features = self.forward_modules[\"preprocessing\"](features)\n        features = self.forward_modules[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/8", "ground_truth": "        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 120, "lineno": 289, "function_name": "patchify", "line_no": 289}}
{"_id": "amazon-science_patchcore-inspection/9", "text": "[\"preadapt_aggregator\"](features)\n\n        if provide_patch_shapes:\n            return _detach(features), patch_shapes\n        return _detach(features)\n\n    def fit(self, training_data):\n        \"\"\"PatchCore training.\n\n        This function computes the embeddings of the training data and fills the\n        memory bank of SPADE.\n        \"\"\"\n        self._fill_memory_bank(training_data)\n\n    def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n\n    def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n\n    def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n\n    def _predict(self, images):\n        \"\"\"Infer score and mask for a batch of images.\"\"\"\n        images = images.to(torch.float).to(self.device)\n        _ = self.forward_modules.eval()\n\n        batchsize = images.shape[0]\n        with torch.no_grad():\n            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n            features = np.asarray(features)\n\n            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n            image_scores = self.patch_maker.unpatch_scores(\n                image_scores, batchsize=batchsize\n            )\n            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n            image_scores = self.patch_maker.score(image_scores)\n\n            patch_scores = self.patch_maker.unpatch_scores(\n                patch_scores, batchsize=batchsize\n            )\n            scales = patch_shapes[0]\n            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n\n            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n\n        return [score for score in image_scores], [mask for mask in masks]\n\n    @staticmethod\n    def _params_file(filepath, prepend=\"\"):\n        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n\n    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n\n    def load_from_path(\n        self,\n        load_path: str,\n        device: torch.device,\n        nn_method: patchcore.common.FaissNN(False, 4),\n        prepend: str = \"\",\n    ) -> None:\n        LOGGER.info(\"Loading and initializing PatchCore.\")\n        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n            patchcore_params = pickle.load(load_file)\n        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n            patchcore_params[\"backbone.name\"]\n        )\n        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n        del patchcore_params[\"backbone.name\"]\n        self.load(**patchcore_params, device=device, nn_method=nn_method)\n\n        self.anomaly_scorer.load(load_path, prepend)\n\n\n# Image handling classes.\nclass PatchMaker:\n    def __init__(self, patchsize, stride=None):\n        self.patchsize = patchsize\n        self.stride = stride\n\n    def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n\n    def unpatch_scores(self, x, batchsize):\n        return x.reshape(batchsize, -1, *x.shape[1:])\n\n    def score(self, x):", "metadata": {"task_id": "amazon-science_patchcore-inspection/9", "ground_truth": "        was_numpy = False\n        if isinstance(x, np.ndarray):\n            was_numpy = True\n            x = torch.from_numpy(x)\n        while x.ndim > 1:\n            x = torch.max(x, dim=-1).values\n        if was_numpy:\n            return x.numpy()\n        return x\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 140, "lineno": 313, "function_name": "score", "line_no": 313}}
{"_id": "amazon-science_patchcore-inspection/10", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/10", "ground_truth": "        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 21, "function_name": "__init__", "line_no": 21}}
{"_id": "amazon-science_patchcore-inspection/11", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):", "metadata": {"task_id": "amazon-science_patchcore-inspection/11", "ground_truth": "        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 29, "function_name": "_index_to_gpu", "line_no": 29}}
{"_id": "amazon-science_patchcore-inspection/12", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):", "metadata": {"task_id": "amazon-science_patchcore-inspection/12", "ground_truth": "        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 38, "function_name": "_index_to_cpu", "line_no": 38}}
{"_id": "amazon-science_patchcore-inspection/13", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):", "metadata": {"task_id": "amazon-science_patchcore-inspection/13", "ground_truth": "        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "_create_index", "line_no": 43}}
{"_id": "amazon-science_patchcore-inspection/14", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/14", "ground_truth": "        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "fit", "line_no": 56}}
{"_id": "amazon-science_patchcore-inspection/15", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/15", "ground_truth": "        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 78, "function_name": "run", "line_no": 78}}
{"_id": "amazon-science_patchcore-inspection/16", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):", "metadata": {"task_id": "amazon-science_patchcore-inspection/16", "ground_truth": "        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "reset_index", "line_no": 94}}
{"_id": "amazon-science_patchcore-inspection/17", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):", "metadata": {"task_id": "amazon-science_patchcore-inspection/17", "ground_truth": "        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 109, "function_name": "_create_index", "line_no": 109}}
{"_id": "amazon-science_patchcore-inspection/18", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC", "metadata": {"task_id": "amazon-science_patchcore-inspection/18", "ground_truth": "        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 132, "function_name": "_reduce", "line_no": 132}}
{"_id": "amazon-science_patchcore-inspection/19", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):", "metadata": {"task_id": "amazon-science_patchcore-inspection/19", "ground_truth": "        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 146, "function_name": "__init__", "line_no": 146}}
{"_id": "amazon-science_patchcore-inspection/20", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):", "metadata": {"task_id": "amazon-science_patchcore-inspection/20", "ground_truth": "        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "forward", "line_no": 156}}
{"_id": "amazon-science_patchcore-inspection/21", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim", "metadata": {"task_id": "amazon-science_patchcore-inspection/21", "ground_truth": "        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 180, "function_name": "forward", "line_no": 180}}
{"_id": "amazon-science_patchcore-inspection/22", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):", "metadata": {"task_id": "amazon-science_patchcore-inspection/22", "ground_truth": "        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 187, "function_name": "__init__", "line_no": 187}}
{"_id": "amazon-science_patchcore-inspection/23", "text": "import copy\nimport os\nimport pickle\nfrom typing import List\nfrom typing import Union\n\nimport faiss\nimport numpy as np\nimport scipy.ndimage as ndimage\nimport torch\nimport torch.nn.functional as F\n\n\nclass FaissNN(object):\n    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n\n    def _gpu_cloner_options(self):\n        return faiss.GpuClonerOptions()\n\n    def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n\n    def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n\n    def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n\n    def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):", "metadata": {"task_id": "amazon-science_patchcore-inspection/23", "ground_truth": "        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 193, "function_name": "convert_to_segmentation", "line_no": 193}}
{"_id": "amazon-science_patchcore-inspection/24", "text": "features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n\n    def _train(self, _index, _features):\n        pass\n\n    def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):", "metadata": {"task_id": "amazon-science_patchcore-inspection/24", "ground_truth": "        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 58, "lineno": 259, "function_name": "forward", "line_no": 259}}
{"_id": "amazon-science_patchcore-inspection/25", "text": "]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/25", "ground_truth": "        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 70, "lineno": 271, "function_name": "feature_dimensions", "line_no": 271}}
{"_id": "amazon-science_patchcore-inspection/26", "text": "index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n\n    def save(self, filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):", "metadata": {"task_id": "amazon-science_patchcore-inspection/26", "ground_truth": "        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 82, "lineno": 278, "function_name": "__init__", "line_no": 278}}
{"_id": "amazon-science_patchcore-inspection/27", "text": ", filename: str) -> None:\n        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n\n    def load(self, filename: str) -> None:\n        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n\n    def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n\n\nclass ApproximateFaissNN(FaissNN):\n    def _train(self, index, features):\n        index.train(features)\n\n    def _gpu_cloner_options(self):\n        cloner = faiss.GpuClonerOptions()\n        cloner.useFloat16 = True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):", "metadata": {"task_id": "amazon-science_patchcore-inspection/27", "ground_truth": "        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 87, "lineno": 285, "function_name": "__call__", "line_no": 285}}
{"_id": "amazon-science_patchcore-inspection/28", "text": " True\n        return cloner\n\n    def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n\n\nclass _BaseMerger:\n    def __init__(self):\n        \"\"\"Merges feature embedding by name.\"\"\"\n\n    def merge(self, features: list):\n        features = [self._reduce(feature) for feature in features]\n        return np.concatenate(features, axis=1)\n\n\nclass AverageMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/28", "ground_truth": "        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 105, "lineno": 305, "function_name": "__init__", "line_no": 305}}
{"_id": "amazon-science_patchcore-inspection/29", "text": "        )\n\n\nclass ConcatMerger(_BaseMerger):\n    @staticmethod\n    def _reduce(features):\n        # NxCxWxH -> NxCWH\n        return features.reshape(len(features), -1)\n\n\nclass Preprocessing(torch.nn.Module):\n    def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/29", "ground_truth": "        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 134, "lineno": 326, "function_name": "fit", "line_no": 326}}
{"_id": "amazon-science_patchcore-inspection/30", "text": " = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n\n    def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n\n\nclass MeanMapper(torch.nn.Module):\n    def __init__(self, preprocessing_dim):\n        super(MeanMapper, self).__init__()\n        self.preprocessing_dim = preprocessing_dim\n\n    def forward(self, features):\n        features = features.reshape(len(features), 1, -1)\n        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n\n\nclass Aggregator(torch.nn.Module):\n    def __init__(self, target_dim):\n        super(Aggregator, self).__init__()\n        self.target_dim = target_dim\n\n    def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"", "metadata": {"task_id": "amazon-science_patchcore-inspection/30", "ground_truth": "        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 152, "lineno": 344, "function_name": "predict", "line_no": 344}}
{"_id": "amazon-science_patchcore-inspection/31", "text": "target_dim)\n        return features.reshape(len(features), -1)\n\n\nclass RescaleSegmentor:\n    def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n\n    def convert_to_segmentation(self, patch_scores):\n\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n\n\nclass NetworkFeatureAggregator(torch.nn.Module):\n    \"\"\"Efficient extraction of network features.\"\"\"\n\n    def __init__(self, backbone, layers_to_extract_from, device):\n        super(NetworkFeatureAggregator, self).__init__()\n        \"\"\"Extraction of network features.\n\n        Runs a network only to the last layer of the list of layers where\n        network features should be extracted from.\n\n        Args:\n            backbone: torchvision.model\n            layers_to_extract_from: [list of str]\n        \"\"\"\n        self.layers_to_extract_from = layers_to_extract_from\n        self.backbone = backbone\n        self.device = device\n        if not hasattr(backbone, \"hook_handles\"):\n            self.backbone.hook_handles = []\n        for handle in self.backbone.hook_handles:\n            handle.remove()\n        self.outputs = {}\n\n        for extract_layer in layers_to_extract_from:\n            forward_hook = ForwardHook(\n                self.outputs, extract_layer, layers_to_extract_from[-1]\n            )\n            if \".\" in extract_layer:\n                extract_block, extract_idx = extract_layer.split(\".\")\n                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n                if extract_idx.isnumeric():\n                    extract_idx = int(extract_idx)\n                    network_layer = network_layer[extract_idx]\n                else:\n                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n            else:\n                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n\n            if isinstance(network_layer, torch.nn.Sequential):\n                self.backbone.hook_handles.append(\n                    network_layer[-1].register_forward_hook(forward_hook)\n                )\n            else:\n                self.backbone.hook_handles.append(\n                    network_layer.register_forward_hook(forward_hook)\n                )\n        self.to(self.device)\n\n    def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n\n    def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n\n\nclass ForwardHook:\n    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n\n    def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n\n\nclass LastLayerToExtractReachedException(Exception):\n    pass\n\n\nclass NearestNeighbourScorer(object):\n    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n\n    def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n\n    def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n\n    @staticmethod\n    def _detection_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n\n    @staticmethod\n    def _index_file(folder, prepend=\"\"):\n        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n\n    @staticmethod\n    def _save(filename, features):\n        if features is None:\n            return\n        with open(filename, \"wb\") as save_file:\n            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def _load(filename: str):\n        with open(filename, \"rb\") as load_file:\n            return pickle.load(load_file)\n\n    def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:", "metadata": {"task_id": "amazon-science_patchcore-inspection/31", "ground_truth": "        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 181, "lineno": 377, "function_name": "save", "line_no": 377}}
